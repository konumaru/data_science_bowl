{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing code_counts.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code_counts.py\n",
    "import pandas as pd\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "def create_counters(fout):\n",
    "    fname = '/kaggle/input/data-science-bowl-2019/'+ fout +'.csv'\n",
    "    df = pd.read_csv(fname)[['timestamp','installation_id','event_code']]\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df = df.sort_values(['installation_id', 'timestamp']).reset_index(drop=True)\n",
    "    df['event_global_enc'] = (df['event_code'] == 2000).astype(int)  \n",
    "    df['event_global_enc'] = df.event_global_enc.cumsum()\n",
    "\n",
    "    agg_df = df.groupby(['event_global_enc','event_code']).agg({'timestamp':'count'}).reset_index()\n",
    "    agg_df.columns = ['event_global_enc','event_code','event_code_count']\n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "    event_codes = ['2000','3010','3110','4020','4021','4030','4035','4070',\n",
    "                   '4090','2020','2030','2040','2050','2080','2083','3020',\n",
    "                   '3021','3120','3121','4010','2060','2070','4031','4025',\n",
    "                   '5000','5010','2081','2025','4022','2010','2035','4040',\n",
    "                   '4100','4110','4045','4095','4220','2075','4230','4235',\n",
    "                   '4080','4050']\n",
    "    dcts = []\n",
    "    for t,g in tqdm(agg_df.groupby('event_global_enc')):\n",
    "        dct = {'event_global_enc': t}\n",
    "        g.index = g['event_code']\n",
    "        g = g['event_code_count'].to_dict()\n",
    "        for k in event_codes:\n",
    "            dct['event_code_' + k] = g.get(int(k), 0)\n",
    "        dcts.append(dct)\n",
    "    pd.DataFrame(dcts).to_csv(fout + '_code_counts.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing create_code_counters_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile create_code_counters_train.py\n",
    "from code_counts import create_counters\n",
    "create_counters('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing create_code_counters_test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile create_code_counters_test.py\n",
    "from code_counts import create_counters\n",
    "create_counters('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 303319/303319 [02:57<00:00, 1712.29it/s]\r\n",
      "100%|███████████████████████████████████| 28445/28445 [00:16<00:00, 1731.33it/s]\r\n",
      "CPU times: user 10 s, sys: 3.56 s, total: 13.6 s\n",
      "Wall time: 6min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "!python create_code_counters_train.py\n",
    "!python create_code_counters_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing preproc.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preproc.py\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import seaborn as sns \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import TimeSeriesSplit, KFold, StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import functools\n",
    "from multiprocessing import Pool\n",
    "import logging\n",
    "import gc\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "def load_csv(filename, fout):\n",
    "    df = pd.read_csv(filename)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df = df.sort_values(['installation_id', 'timestamp']).reset_index(drop=True)\n",
    "    \n",
    "    df['correct'] = df['event_data'].str.contains('\"correct\":true').astype(int)\n",
    "    df['incorrect'] = df['event_data'].str.contains('\"correct\":false').astype(int)\n",
    "    df.drop(['event_data'], axis=1, inplace=True)\n",
    "    gc.collect()\n",
    "    df['super_token'] = df['title'] + df['event_code'].astype(str)+df['correct'].astype(str)+df['incorrect'].astype(str)\n",
    "    df['super_token'] = df['super_token'].str.replace(' ','').str.replace('-','')\n",
    "    \n",
    "    df['attempt'] = (((df.event_code == 4100) & (df.title != 'Bird Measurer (Assessment)')) |\\\n",
    "                     ((df.event_code == 4110) & (df.title == 'Bird Measurer (Assessment)'))) &\\\n",
    "                    (df['type'] == 'Assessment')\n",
    "    df['attempt'] = df['attempt'].astype(int)\n",
    "    df['correct'] = df['correct'] * df['attempt']\n",
    "    df['incorrect'] = df['incorrect'] * df['attempt']\n",
    "    \n",
    "    df['start_event'] = (df['event_code'] == 2000).astype(int)  \n",
    "    df['start_assessment'] = (df['type'] == 'Assessment').astype(int) * df['start_event']\n",
    "    df['end_event'] = df.start_event.shift(-1, fill_value=1)\n",
    "    df['event_global_enc'] = df.start_event.cumsum()\n",
    "    gc.collect()\n",
    "    \n",
    "    agg_df = df.groupby('event_global_enc').agg({'installation_id':'first', 'correct': 'sum', 'incorrect': 'sum', 'timestamp': ['min','max']}).reset_index()\n",
    "    agg_df.columns = ['event_global_enc', 'installation_id', 'correct_attempts', 'incorrect_attempts', 'ts_min','ts_max']\n",
    "    agg_df['game_duration'] = (agg_df['ts_min'] - agg_df['ts_min'].shift(1)).dt.days*3600*24 +\\\n",
    "    (agg_df['ts_min'] - agg_df['ts_min'].shift(1)).dt.seconds +\\\n",
    "    (agg_df['ts_min'] - agg_df['ts_min'].shift(1)).dt.microseconds / 1e6\n",
    "    agg_df['gs'] = 1\n",
    "    agg_df['gs'] = agg_df.groupby('installation_id')['gs'].transform(pd.Series.cumsum)\n",
    "    agg_df.loc[agg_df.gs==1,'game_duration'] = 0\n",
    "    agg_df['game_duration'] = np.log1p(agg_df['game_duration'])\n",
    "    \n",
    "    aggcols = ['event_global_enc', 'correct_attempts', 'incorrect_attempts', 'game_duration']\n",
    "    df = df.merge(agg_df[aggcols], on='event_global_enc', how='left')\n",
    "    del agg_df\n",
    "    gc.collect()\n",
    "    \n",
    "    assessments = ['Bird Measurer (Assessment)', 'Cart Balancer (Assessment)', \n",
    "                   'Cauldron Filler (Assessment)', 'Chest Sorter (Assessment)', 'Mushroom Sorter (Assessment)']\n",
    "    assessment_fts = []\n",
    "    for a in assessments:\n",
    "        feat1 = a.replace(' ','')+'_correct'\n",
    "        feat2 = a.replace(' ','')+'_incorrect'\n",
    "        assessment_fts.append(feat1)\n",
    "        assessment_fts.append(feat2)\n",
    "        df[feat1] = 0\n",
    "        df[feat2] = 0\n",
    "        df.loc[df.title == a,feat1] = df.loc[df.title==a].groupby(['installation_id'])['correct'].transform(pd.Series.cumsum)\n",
    "        df.loc[df.title == a,feat2] = df.loc[df.title==a].groupby(['installation_id'])['incorrect'].transform(pd.Series.cumsum)\n",
    "    \n",
    "    df['metric_point'] = df['start_assessment'] * (df.correct_attempts + df.incorrect_attempts > 0).astype(int)\n",
    "    df['metric_point_inference'] = df.installation_id != df.installation_id.shift(-1, fill_value='')\n",
    "    gc.collect()\n",
    "    \n",
    "    ret_columns = ['installation_id', 'title', 'super_token', 'event_code', 'game_duration',\n",
    "                   'correct_attempts', 'incorrect_attempts', \n",
    "                   'metric_point', 'metric_point_inference', 'event_global_enc'\n",
    "                  ] + assessment_fts\n",
    "    df = df[ret_columns]\n",
    "    gc.collect()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_text_file(fname, fout):\n",
    "    train = load_csv(fname, fout)\n",
    "    \n",
    "    #if fout == 'test':\n",
    "    #    train['mp'] = train['metric_point']\n",
    "    #    train['mp'] = train.groupby(['installation_id'])['mp'].transform(pd.Series.cumsum)\n",
    "    #    train.loc[(train.metric_point == 1) & (train.mp > 1),'metric_point'] = 0\n",
    "    #    train.drop(['mp'], axis=1, inplace=True)\n",
    "    \n",
    "    train['event_idx'] = 1\n",
    "    train['event_idx'] = train.groupby(['installation_id'])['event_idx'].transform(pd.Series.cumsum)\n",
    "\n",
    "    texts = []\n",
    "    if fout == 'train':\n",
    "        labels = {}\n",
    "        for i,q in enumerate(train.super_token.unique()):\n",
    "            labels[q] = i + 1\n",
    "        with open('txt_labels.pickle', 'wb') as handle:\n",
    "            pickle.dump(labels,handle)\n",
    "    else:\n",
    "        with open('txt_labels.pickle', 'rb') as handle:\n",
    "            labels = pickle.load(handle)\n",
    "    train['super_token'] = train['super_token'].map(lambda x: labels.get(x, 0)).astype(np.int16)\n",
    "\n",
    "    for ix in tqdm(train.loc[train.metric_point_inference==1].index):\n",
    "        point_idx = train.iloc[ix].event_idx\n",
    "        texts.append(train.iloc[(ix-point_idx+1):(ix+1)]['super_token'].values.tolist())\n",
    "        \n",
    "    np.save(fout + '_mpi', np.array(texts))\n",
    "    \n",
    "    texts = []\n",
    "    for ix in tqdm(train.loc[train.metric_point==1].index):\n",
    "        point_idx = train.iloc[ix].event_idx\n",
    "        texts.append(train.iloc[(ix-point_idx+1):(ix+1)]['super_token'].values.tolist())\n",
    "    np.save(fout + '_mp', np.array(texts))\n",
    "        \n",
    "    assessments = ['Bird Measurer (Assessment)', 'Cart Balancer (Assessment)', \n",
    "               'Cauldron Filler (Assessment)', 'Chest Sorter (Assessment)', 'Mushroom Sorter (Assessment)']\n",
    "    assessment_fts = []\n",
    "    for a in assessments:\n",
    "        feat1 = a.replace(' ','')+'_correct'\n",
    "        feat2 = a.replace(' ','')+'_incorrect'\n",
    "        feat3 = a.replace(' ','')+'_rate'\n",
    "        assessment_fts.append(feat1)\n",
    "        assessment_fts.append(feat2)\n",
    "        assessment_fts.append(feat3)\n",
    "        train[feat3] = 0\n",
    "        train.loc[train[feat1]+train[feat2] > 0,feat3] = train.loc[train[feat1]+train[feat2] > 0,feat1] / \\\n",
    "        (train.loc[train[feat1]+train[feat2] > 0,feat1] + train.loc[train[feat1]+train[feat2] > 0,feat2])\n",
    "        \n",
    "    train['assessment_rate'] = 0\n",
    "    train.loc[train.correct_attempts + train.incorrect_attempts > 0, 'assessment_rate'] = \\\n",
    "                  train.correct_attempts / (train.correct_attempts + train.incorrect_attempts)\n",
    "    \n",
    "    usefull_fts = ['installation_id', 'title', 'assessment_rate', \n",
    "                   'game_duration', 'correct_attempts', 'incorrect_attempts',\n",
    "                   'metric_point', 'metric_point_inference', 'event_global_enc'] + assessment_fts\n",
    "\n",
    "    train = train.loc[train.event_code == 2000, usefull_fts].reset_index(drop=True)\n",
    "    gc.collect()\n",
    "    \n",
    "    ecc = pd.read_csv(fout + '_code_counts.csv')\n",
    "    train = train.merge(ecc, on='event_global_enc', how='left').drop(['event_global_enc'], axis=1)\n",
    "    del ecc\n",
    "    gc.collect()\n",
    "    \n",
    "    train['event_idx'] = 1\n",
    "    train['event_idx'] = train.groupby(['installation_id'])['event_idx'].transform(pd.Series.cumsum)\n",
    "    \n",
    "    train['label'] = 0\n",
    "    train.loc[(train.incorrect_attempts >= 2) & (train.correct_attempts > 0),'label'] = 1\n",
    "    train.loc[(train.incorrect_attempts == 1) & (train.correct_attempts > 0),'label'] = 2\n",
    "    train.loc[(train.incorrect_attempts == 0) & (train.correct_attempts > 0),'label'] = 3\n",
    "    train['all_attempts'] = train.incorrect_attempts + train.correct_attempts\n",
    "    \n",
    "    train.to_csv(fout+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing text_processing_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile text_processing_train.py\n",
    "\n",
    "from preproc import create_text_file\n",
    "\n",
    "create_text_file('/kaggle/input/data-science-bowl-2019/train.csv', 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing text_processing_test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile text_processing_test.py\n",
    "\n",
    "from preproc import create_text_file\n",
    "\n",
    "create_text_file('/kaggle/input/data-science-bowl-2019/test.csv', 'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 17000/17000 [00:13<00:00, 1299.86it/s]\r\n",
      "100%|███████████████████████████████████| 17690/17690 [00:15<00:00, 1175.42it/s]\r\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 1278.19it/s]\r\n",
      "100%|█████████████████████████████████████| 2018/2018 [00:01<00:00, 1224.13it/s]\r\n",
      "CPU times: user 11.8 s, sys: 4.76 s, total: 16.5 s\n",
      "Wall time: 8min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python text_processing_train.py\n",
    "!python text_processing_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_tfidf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_tfidf.py\n",
    "\n",
    "import numpy as np \n",
    "import gc\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from scipy.sparse import save_npz\n",
    "\n",
    "texts = np.load('train_mpi.npy',allow_pickle=True).tolist() + np.load('test_mpi.npy',allow_pickle=True).tolist()\n",
    "for i,t in tqdm(enumerate(texts)):\n",
    "    texts[i] = ' '.join(['q' + str(q) for q in t])\n",
    "print(len(texts), len(texts[0]), texts[0][:10])\n",
    "\n",
    "vectorizer = TfidfVectorizer(input='content', encoding='utf-8', decode_error='strict', strip_accents=None, \n",
    "                             lowercase=False, preprocessor=None, tokenizer=None, analyzer='word', stop_words=None, \n",
    "                             token_pattern='\\S+', ngram_range=(1, 3), max_df=0.99, min_df=100, max_features=None, \n",
    "                             vocabulary=None, binary=False, norm='l2', use_idf=True, \n",
    "                             smooth_idf=True, sublinear_tf=False)\n",
    "vectorizer.fit(texts)\n",
    "del texts\n",
    "gc.collect()\n",
    "\n",
    "with open('vectorizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(vectorizer,handle)\n",
    "\n",
    "for f in ['train_mp','test_mp','test_mpi']:\n",
    "    texts = np.load(f + '.npy',allow_pickle=True).tolist()\n",
    "    for i,t in tqdm(enumerate(texts)):\n",
    "        texts[i] = ' '.join(['q' + str(q) for q in t])\n",
    "    texts = vectorizer.transform(texts)\n",
    "    save_npz(f, texts)\n",
    "    del texts\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000it [00:04, 3711.57it/s]\r\n",
      "18000 5012 q1 q2 q3 q\r\n",
      "17690it [00:18, 965.51it/s]\r\n",
      "2018it [00:01, 1345.65it/s]\r\n",
      "1000it [00:00, 2277.32it/s]\r\n",
      "CPU times: user 6.18 s, sys: 2.34 s, total: 8.52 s\n",
      "Wall time: 4min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python train_tfidf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_model.py\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import seaborn as sns \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import TimeSeriesSplit, KFold, StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import functools\n",
    "from multiprocessing import Pool\n",
    "import logging\n",
    "import gc\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm_notebook\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "\n",
    "SEED = 239\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "q_train = scipy.sparse.load_npz('train_mp.npz')\n",
    "q_train_tst = scipy.sparse.load_npz('test_mp.npz')\n",
    "\n",
    "q_train = scipy.sparse.vstack([q_train, q_train_tst])\n",
    "del q_train_tst\n",
    "gc.collect()\n",
    "q_test = scipy.sparse.load_npz('test_mpi.npz')\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "train2 = pd.read_csv('train.csv')\n",
    "\n",
    "print(train2.shape, test.shape, q_train.shape, q_test.shape,      train2.loc[train2.metric_point==1].shape, test.loc[test.metric_point==1].shape)\n",
    "\n",
    "train_ids = train2.installation_id.unique()\n",
    "test_ids = test.loc[test.metric_point_inference==1,'installation_id'].values\n",
    "train2 = pd.concat([train2,test], sort=False, ignore_index=True).reset_index(drop=True)\n",
    "del test\n",
    "gc.collect()\n",
    "\n",
    "train2['event_idx'] = 1\n",
    "train2['event_idx'] = train2.groupby(['installation_id'])['event_idx'].transform(pd.Series.cumsum)\n",
    "\n",
    "event_codes = ['3010','3110','4020','4021','4030','4035',\n",
    "               '4090','2020','2030','2040','2050','2080','2083',\n",
    "               '3021','3120','4010','2060','2070','4031','4025',\n",
    "               '5000','5010','2081','2025','4022','2010','2035','4040',\n",
    "               '4100','4110','4045','4095','4220','2075','4230','4235',\n",
    "               '4080','4050']\n",
    "#event_codes = ['4070','3020','3121','4020','3120','2030','4035','4030']\n",
    "code_counts_cols = ['event_code_' + q for q in event_codes]\n",
    "\n",
    "assessments = ['Bird Measurer (Assessment)', 'Cart Balancer (Assessment)', \n",
    "               'Cauldron Filler (Assessment)', 'Chest Sorter (Assessment)', 'Mushroom Sorter (Assessment)']\n",
    "assessment_fts = ['BirdMeasurer(Assessment)_correct',\n",
    " 'BirdMeasurer(Assessment)_incorrect',\n",
    " 'BirdMeasurer(Assessment)_rate',\n",
    " 'CartBalancer(Assessment)_correct',\n",
    " 'CartBalancer(Assessment)_incorrect',\n",
    " 'CartBalancer(Assessment)_rate',\n",
    " 'CauldronFiller(Assessment)_correct',\n",
    " 'CauldronFiller(Assessment)_incorrect',\n",
    " 'CauldronFiller(Assessment)_rate',\n",
    " 'ChestSorter(Assessment)_correct',\n",
    " 'ChestSorter(Assessment)_incorrect',\n",
    " 'ChestSorter(Assessment)_rate',\n",
    " 'MushroomSorter(Assessment)_correct',\n",
    " 'MushroomSorter(Assessment)_incorrect',\n",
    " 'MushroomSorter(Assessment)_rate']\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "labelers = {}\n",
    "cat_cols = ['title']\n",
    "\n",
    "print(\"Process categorical features:\")\n",
    "for col_name in cat_cols:\n",
    "    labelers[col_name] = {x:i+1 for i, x in enumerate(train2.loc[:, col_name].unique())}\n",
    "    train2[col_name] = train2[col_name].apply(lambda x: labelers[col_name].get(x, 0))\n",
    "    \n",
    "cat_sizes_map = {col_name: len(labeler)+1 for col_name, labeler in labelers.items()}\n",
    "cat_sizes_map\n",
    "\n",
    "#'correct_attempts',\n",
    "train2['correct_attempts'].fillna(0, inplace=True)\n",
    "num_cols = ['incorrect_attempts','all_attempts','game_duration'] + assessment_fts + code_counts_cols\n",
    "\n",
    "feat_scalers = {}\n",
    "high_cups = {}\n",
    "for f in num_cols:\n",
    "    feat_scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "    high_cup = np.percentile(train2.loc[train2.metric_point==1,f].values, 99)\n",
    "    high_cups[f] = high_cup\n",
    "    print(f, high_cup)\n",
    "    train2.loc[train2[f] > high_cup, f] = high_cup + 1\n",
    "    feat_scaler.fit(train2[f].fillna(0).astype(\"float32\").values.reshape(-1,1))\n",
    "    train2[f] = feat_scaler.transform(train2[f].fillna(0).astype(\"float32\").values.reshape(-1,1))\n",
    "    feat_scalers[f] = feat_scaler\n",
    "print(high_cups)\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "first_dim = train2.loc[train2.metric_point==1,:].shape[0]\n",
    "seq_len = 64\n",
    "num_cols = ['correct_attempts', 'incorrect_attempts', 'assessment_rate'] + code_counts_cols + ['game_duration', \n",
    "            'current_assessment_correct', 'current_assessment_incorrect', 'current_assessment_rate']\n",
    "\n",
    "matrix_titles = np.zeros((first_dim, seq_len))\n",
    "matrix_numericals = np.zeros((first_dim, seq_len, len(num_cols)))\n",
    "\n",
    "instids = train2.loc[train2.metric_point==1,'installation_id'].values\n",
    "j = 0\n",
    "for ix in tqdm_notebook(train2.loc[train2.metric_point==1].index):\n",
    "    point_idx = train2.iloc[ix].event_idx\n",
    "    cur_title = train2.iloc[ix].title\n",
    "    f1 = np.min([point_idx, seq_len])\n",
    "    matrix_titles[j,(seq_len - f1):] = train2.iloc[(ix-f1+1):(ix+1)]['title'].values\n",
    "    for k, f in enumerate(num_cols[:-3]):\n",
    "        matrix_numericals[j,(seq_len - f1):,k] = train2.iloc[(ix-f1+1):(ix+1)][f].values\n",
    "    assessment_idx = 0\n",
    "    for ia,a in enumerate(assessments):\n",
    "        if labelers['title'][a] == cur_title:\n",
    "            assessment_idx = ia\n",
    "    cur_fts = [assessment_fts[3*assessment_idx], assessment_fts[3*assessment_idx+1],assessment_fts[3*assessment_idx+2]]\n",
    "    matrix_numericals[j,(seq_len - f1):,-3:] = train2.iloc[(ix-f1+1):(ix+1)][cur_fts].values\n",
    "    j += 1\n",
    "to_zero_cols_count = len(['correct_attempts', 'incorrect_attempts', 'assessment_rate'] + code_counts_cols)\n",
    "matrix_numericals[:,-1,:to_zero_cols_count] = 0 #[[0]*to_zero_cols_count]*matrix_numericals.shape[0]\n",
    "#matrix_numericals[:,-6:,3:to_zero_cols_count] = 0 #[[0]*to_zero_cols_count]*matrix_numericals.shape[0]\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "cat_cols = ['title']\n",
    "for col_name in cat_cols:\n",
    "    test[col_name] = test[col_name].apply(lambda x: labelers[col_name].get(x, 0))\n",
    "\n",
    "test['correct_attempts'].fillna(0, inplace=True)\n",
    "num_cols = ['incorrect_attempts','game_duration']  + assessment_fts + code_counts_cols\n",
    "for f in num_cols:\n",
    "    high_cup = high_cups[f]\n",
    "    test.loc[test[f] > high_cup, f] = high_cup + 1\n",
    "    test[f] = feat_scalers[f].transform(test[f].fillna(0).astype(\"float32\").values.reshape(-1,1))\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "first_dim = test.loc[test.metric_point_inference==1,:].shape[0]\n",
    "seq_len = 64\n",
    "num_cols = ['correct_attempts', 'incorrect_attempts', 'assessment_rate'] + code_counts_cols + ['game_duration', \n",
    "            'current_assessment_correct', 'current_assessment_incorrect', 'current_assessment_rate']\n",
    "\n",
    "matrix_titles_test = np.zeros((first_dim, seq_len))\n",
    "matrix_numericals_test = np.zeros((first_dim, seq_len, len(num_cols)))\n",
    "\n",
    "j = 0\n",
    "for ix in tqdm_notebook(test.loc[test.metric_point_inference==1,:].index):\n",
    "    point_idx = test.iloc[ix].event_idx\n",
    "    cur_title = test.iloc[ix].title\n",
    "    f1 = np.min([point_idx, seq_len])\n",
    "    matrix_titles_test[j,(seq_len - f1):] = test.iloc[(ix-f1+1):(ix+1)]['title'].values\n",
    "    for k, f in enumerate(num_cols[:-3]):\n",
    "        matrix_numericals_test[j,(seq_len - f1):,k] = test.iloc[(ix-f1+1):(ix+1)][f].values\n",
    "    assessment_idx = 0\n",
    "    for ia,a in enumerate(assessments):\n",
    "        if labelers['title'][a] == cur_title:\n",
    "            assessment_idx = ia\n",
    "    cur_fts = [assessment_fts[3*assessment_idx], assessment_fts[3*assessment_idx+1],assessment_fts[3*assessment_idx+2]]\n",
    "    matrix_numericals_test[j,(seq_len - f1):,-3:] = test.iloc[(ix-f1+1):(ix+1)][cur_fts].values\n",
    "    j += 1\n",
    "\n",
    "matrix_numericals_test[:,-1,:to_zero_cols_count] = 0 #[[0]*to_zero_cols_count]*matrix_numericals_test.shape[0]\n",
    "#matrix_numericals_test[:,-6:,3:to_zero_cols_count] = 0\n",
    "print(matrix_numericals_test.shape, matrix_titles_test.shape)\n",
    "\n",
    "del test\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from numba import jit \n",
    "from functools import partial\n",
    "\n",
    "\n",
    "@jit\n",
    "def qwk3(a1, a2):\n",
    "    assert(len(a1) == len(a2))\n",
    "    a1 = np.asarray(a1, dtype=int)\n",
    "    a2 = np.asarray(a2, dtype=int)\n",
    "\n",
    "    hist1 = np.zeros((4, ))\n",
    "    hist2 = np.zeros((4, ))\n",
    "\n",
    "    o = 0\n",
    "    for k in range(a1.shape[0]):\n",
    "        i, j = a1[k], a2[k]\n",
    "        hist1[i] += 1\n",
    "        hist2[j] += 1\n",
    "        o +=  (i - j) * (i - j)\n",
    "\n",
    "    e = 0\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n",
    "\n",
    "    e = e / a1.shape[0]\n",
    "\n",
    "    return 1 - o / e     \n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "\n",
    "import itertools\n",
    "from keras import Model\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.callbacks import *\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import backend as K\n",
    "from keras.utils import Sequence\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from sklearn.metrics import mean_squared_error, log_loss, mean_absolute_error\n",
    "\n",
    "import scipy as sp\n",
    "\n",
    "class FeatureSequence(Sequence):\n",
    "    def __init__(self, Xs, Ys, batch_size, shuffle=False):\n",
    "        self.Xs = Xs\n",
    "        self.Ys = Ys\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.inx = np.arange(self.Xs[0].shape[0])\n",
    "        self.shuffle = shuffle \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.inx)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(self.inx.shape[0] / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        batch_inx = self.inx[i*self.batch_size:(i+1)*self.batch_size]\n",
    "        if self.Ys is None:\n",
    "            return [X[batch_inx] for X in self.Xs], None\n",
    "        return [X[batch_inx] for X in self.Xs], [Y[batch_inx] for Y in self.Ys]\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.inx)\n",
    "            \n",
    "class OptimizedRounder(object):\n",
    "    def __init__(self, samples):\n",
    "        self.coef_ = 0\n",
    "        self.samples = np.array(samples).astype(np.int32)\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            else:\n",
    "                X_p[i] = 3\n",
    "        tts = []\n",
    "        for i in range(self.samples.shape[0]):\n",
    "            tts.append(qwk3(np.array(y)[self.samples[i]], np.array(X_p)[self.samples[i]]))\n",
    "        ll = np.median(tts)  \n",
    "        return -ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        initial_coef = [1.12232214,1.73925866,2.22506454]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead',\n",
    "                                          options = {'maxiter':1e7, 'xatol':1e-4}\n",
    "                                         )\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            else:\n",
    "                X_p[i] = 3\n",
    "        return X_p\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']\n",
    "            \n",
    "class KappaEvaluationSeq(Callback):\n",
    "    def __init__(self, X_seq, Y, Y2, samples, name, interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.X_seq = X_seq\n",
    "        self.Y, self.Y2 = Y, Y2\n",
    "        self.samples = samples.astype(np.int32)\n",
    "        self.name = name\n",
    "        self.interval = interval\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred1 = self.model.predict_generator(self.X_seq, steps=len(self.X_seq), \n",
    "                                                            use_multiprocessing=False, workers=1, \n",
    "                                                            max_queue_size=2*4)\n",
    "            y_pred1 = y_pred1.ravel()\n",
    "            vmae = mean_absolute_error(self.Y, y_pred1)     \n",
    "            \n",
    "            optR = OptimizedRounder(self.samples)\n",
    "            optR.fit(y_pred1, self.Y)\n",
    "            coefficients = optR.coefficients()\n",
    "            y_pred = optR.predict(y_pred1, coefficients)     \n",
    "            tts = []\n",
    "            for i in range(self.samples.shape[0]):\n",
    "                tts.append(qwk3(np.array(self.Y)[self.samples[i]], np.array(y_pred.astype(int))[self.samples[i]]))\n",
    "            kapa = np.median(tts) \n",
    "            #kapa = qwk3(self.Y, y_pred.astype(int))\n",
    "            \n",
    "            logs[self.name+\"_kappa\"] = kapa\n",
    "            logs[self.name+\"_mae\"] = vmae\n",
    "            coefs = (\"[{:.4f},{:.4f},{:.4f}]\".format(coefficients[0],coefficients[1],coefficients[2]))\n",
    "            print((self.name+\"_kappa: {:.4f}; \"+self.name+\"_mae: {:.4f}; \"+coefs).format(kapa,vmae))\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras import constraints\n",
    "from keras import regularizers\n",
    "from keras.engine import InputSpec, Layer\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Note: The layer has been tested with Keras 2.0.6\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.squeeze(K.dot(x, K.expand_dims(self.W)), axis=-1)\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n",
    "    \n",
    "class AttentionWeightedAverage(Layer):\n",
    "    \"\"\"\n",
    "    Computes a weighted average of the different channels across timesteps.\n",
    "    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.get('uniform')\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(AttentionWeightedAverage, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init)\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttentionWeightedAverage, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # computes a probability distribution over the timesteps\n",
    "        # uses 'max trick' for numerical stability\n",
    "        # reshape is done to avoid issue with Tensorflow\n",
    "        # and 1-dimensional weights\n",
    "        logits = K.dot(x, self.W)\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "\n",
    "        # masked timesteps have zero weight\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            ai = ai * mask\n",
    "        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
    "        weighted_input = x * K.expand_dims(att_weights)\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        if self.return_attention:\n",
    "            return [result, att_weights]\n",
    "        return result\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "from keras.callbacks import *\n",
    "import math\n",
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.learning_rate, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.learning_rate, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('learning_rate', []).append(K.get_value(self.model.optimizer.learning_rate))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.learning_rate, self.clr())\n",
    "from typing import Optional, Callable, List, Dict, MutableSequence\n",
    "from collections import OrderedDict\n",
    "from keras.optimizers import Optimizer\n",
    "\n",
    "CPU_CORES = 1\n",
    "\n",
    "class StochasticEnsembling(Callback):\n",
    "\n",
    "    def __init__(self, seqs_dict: Dict[str, Sequence], cycle_len: int, iter_per_epoch: int,\n",
    "                 alpha1: float = 0.0, alpha2: float = 0.0, lr_schedule_mode: Optional[str] = None,\n",
    "                 swa_cycle_start_inx: int = -1, encoder_layers_out: List[str] = [],\n",
    "                 save_swa_model: bool = False, save_se_weights: bool = False,\n",
    "                 folder: str = \"\", model_name: str = \"\", verbose: int = 1):\n",
    "        \"\"\"\n",
    "        General implementation of Stochastic Weight Averaging (SWA) and Snapshot Ensembling (SE) with variety\n",
    "        of available LR schedules modes.\n",
    "        SWA   https://arxiv.org/abs/1803.05407\n",
    "        FGE   https://arxiv.org/abs/1802.10026\n",
    "        SE    https://arxiv.org/abs/1704.00109\n",
    "        CLR   https://arxiv.org/abs/1506.01186\n",
    "        CALR  https://arxiv.org/abs/1811.00641 (adapted)\n",
    "        :param seqs_dict: Dict of sequences and their names for predicting\n",
    "        :param cycle_len: length of the cycle\n",
    "        :param iter_per_epoch: iterations per epoch\n",
    "        :param alpha1: alpha1 param, usually max lr\n",
    "        :param alpha2: alpha2 param, usually min lr\n",
    "        :param lr_schedule_mode: one of of the [None, \"swa\", \"fge\", \"se\", \"clr\", \"clr2\", \"calr\"]\n",
    "        :param swa_cycle_start_inx: after which cycle start making snapshots and update SWA weights.\n",
    "                                If equals 0, makes it instantly before training\n",
    "        :param encoder_layers_out: list of encoder layers names to use as additional models outputs\n",
    "        :param save_se_weights: should weights of each snapshot be saved\n",
    "        :param save_swa_model: should SWA final model be saved\n",
    "        :param folder: saving directory\n",
    "        :param model_name: model name\n",
    "        :param verbose: verbose\n",
    "        \"\"\"\n",
    "        super(StochasticEnsembling, self).__init__()\n",
    "        self.alpha1 = alpha1\n",
    "        self.alpha2 = alpha2\n",
    "        self.cycle_len = cycle_len\n",
    "        self.iter_per_epoch = iter_per_epoch\n",
    "        self.iter_per_cycle = self.cycle_len * self.iter_per_epoch\n",
    "        self.cycle_num = 0\n",
    "        self.clr_iterations = 0\n",
    "        self.current_epoch = 0\n",
    "        self.lr_schedule_mode = lr_schedule_mode\n",
    "        self.swa_cycle_start_inx = swa_cycle_start_inx\n",
    "        self.encoder_layers_out = encoder_layers_out\n",
    "        self.model_wfo = None\n",
    "\n",
    "        self.model_counts = 0\n",
    "        self.seqs_dict = seqs_dict\n",
    "        self.probs_dict = {k: [] for k in self.seqs_dict.keys()}\n",
    "        self.features_dict = {k: [] for k in self.seqs_dict.keys()}\n",
    "\n",
    "        self.save_se_weights = save_se_weights\n",
    "        self.save_swa_model = save_swa_model\n",
    "        self.folder = folder\n",
    "        self.model_name = model_name\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.swa_weights = []\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        if self.save_se_weights:\n",
    "            self.model.save_weights(self.folder + self.model_name + \"_se_weights_init.h5\")\n",
    "\n",
    "        if self.swa_cycle_start_inx == 0:\n",
    "            self.snapshot_predict()\n",
    "            self.swa_weights = self.model.get_weights()\n",
    "            self.model_counts += 1\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.swa_cycle_start_inx >= 0:\n",
    "            self.model.set_weights(self.swa_weights)\n",
    "        if self.save_swa_model:\n",
    "            self.model.save(self.folder + self.model_name + \"_swa_model.h5\")\n",
    "\n",
    "        if len(self.encoder_layers_out) > 0:\n",
    "            self.model_wfo = Model(inputs=self.model.inputs,\n",
    "                                   outputs=self.model.outputs + [self.model.get_layer(name=layer_name).output\n",
    "                                                                 for layer_name in self.encoder_layers_out])\n",
    "\n",
    "            for seq_name, seq in self.seqs_dict.items():\n",
    "                pred_outs = self.model_wfo.predict_generator(seq, steps=len(seq),\n",
    "                                                             use_multiprocessing=False, workers=CPU_CORES,\n",
    "                                                             max_queue_size=2 * CPU_CORES + 2,\n",
    "                                                             verbose=0)\n",
    "                self.probs_dict[seq_name].append(pred_outs[0])\n",
    "                self.features_dict[seq_name] = pred_outs[1:]\n",
    "        else:\n",
    "            self.snapshot_predict()\n",
    "        self.model_counts += 1\n",
    "\n",
    "        for seq_name, probs in self.probs_dict.items():\n",
    "            self.probs_dict[seq_name] = np.concatenate(probs, axis=-1)\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.current_epoch = epoch\n",
    "        if self.lr_schedule_mode == \"se\":\n",
    "            lr = self._se_schedule()\n",
    "            K.set_value(self.model.optimizer.learning_rate, lr)\n",
    "            if self.verbose > 0:\n",
    "                print(\"Modifying learning rate to {}\".format(str(lr)))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (self.lr_schedule_mode == \"fge\") and (self._t_cycle() != 0.5):\n",
    "            return\n",
    "        elif self._t_cycle() != 1.0:\n",
    "            return\n",
    "\n",
    "        self.cycle_num += 1\n",
    "        if self.verbose > 0:\n",
    "            print(\"Latest lr: {:.5f}\".format(K.get_value(self.model.optimizer.learning_rate)))\n",
    "            if self.lr_schedule_mode == \"fge\":\n",
    "                print(\"Reached half of {} cycle\".format(str(self.cycle_num)))\n",
    "            else:\n",
    "                print(\"Reached {} cycle\".format(str(self.cycle_num)))\n",
    "\n",
    "        if self.save_se_weights:\n",
    "            self.model.save_weights(self.folder + self.model_name + \"_se_weights_\" + str(self.model_counts) + \".h5\")\n",
    "\n",
    "        if (self.swa_cycle_start_inx >= 0) and (self.cycle_num >= self.swa_cycle_start_inx):\n",
    "            self.snapshot_predict()\n",
    "            self.swa_weights_update()\n",
    "            self.model_counts += 1\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        if (self.lr_schedule_mode is None) or (self.lr_schedule_mode == \"se\"):\n",
    "            return\n",
    "\n",
    "        if self.lr_schedule_mode == \"clr\":\n",
    "            lr = self._clr_schedule()\n",
    "        elif self.lr_schedule_mode == \"clr2\":\n",
    "            lr = self._clr2_schedule()\n",
    "        elif self.lr_schedule_mode == \"calr\":\n",
    "            lr = self._calr_schedule()\n",
    "        elif self.lr_schedule_mode == \"fge\":\n",
    "            lr = self._fge_schedule()\n",
    "        elif self.lr_schedule_mode == \"swa\":\n",
    "            lr = self._swa_schedule()\n",
    "        else:\n",
    "            raise ValueError(\"Unknown schedule mode: \" + str(self.lr_schedule_mode))\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "    def _swa_schedule(self):\n",
    "        return (1 - self._t_cycle()) * self.alpha1 + self._t_cycle() * self.alpha2\n",
    "\n",
    "    def _fge_schedule(self):\n",
    "        if self._t_cycle() <= 0.5:\n",
    "            return ((1.0 - 2.0 * self._t_cycle()) * self.alpha1) + (2.0 * self._t_cycle() * self.alpha2)\n",
    "        else:\n",
    "            return ((2.0 - 2.0 * self._t_cycle()) * self.alpha2) + ((2.0 * self._t_cycle() - 1.0) * self.alpha1)\n",
    "\n",
    "    def _se_schedule(self):\n",
    "        lr = math.pi * (self.current_epoch % self.cycle_len) / self.cycle_len\n",
    "        lr = self.alpha1 / 2 * (math.cos(lr) + 1)\n",
    "        return lr\n",
    "\n",
    "    def _clr_schedule(self):\n",
    "        if self._t_cycle() <= 0.5:\n",
    "            return ((1.0 - 2.0 * self._t_cycle()) * self.alpha2) + (2.0 * self._t_cycle() * self.alpha1)\n",
    "        else:\n",
    "            return ((2.0 - 2.0 * self._t_cycle()) * self.alpha1) + ((2.0 * self._t_cycle() - 1.0) * self.alpha2)\n",
    "\n",
    "    def _clr2_schedule(self):\n",
    "        decay = 1 / (2 ** self.cycle_num)\n",
    "        if self._t_cycle() <= 0.5:\n",
    "            return ((1.0 - 2.0 * self._t_cycle()) * self.alpha2) + (2.0 * self._t_cycle() * self.alpha1) * decay\n",
    "        else:\n",
    "            return ((2.0 - 2.0 * self._t_cycle()) * self.alpha1) * decay + ((2.0 * self._t_cycle() - 1.0) * self.alpha2)\n",
    "\n",
    "    def _calr_schedule(self):\n",
    "        decay = ((self.cycle_len + 1) / 10) ** (self.current_epoch % self.cycle_len)  # TODO find something better\n",
    "        if self._t_epoch() <= 0.5:\n",
    "            return ((1.0 - 2.0 * self._t_epoch()) * self.alpha2) + (2.0 * self._t_epoch() * self.alpha1) * decay\n",
    "        else:\n",
    "            return ((2.0 - 2.0 * self._t_epoch()) * self.alpha1) * decay + ((2.0 * self._t_epoch() - 1.0) * self.alpha2)\n",
    "\n",
    "    def _t_cycle(self):\n",
    "        return (((self.clr_iterations - 1) % self.iter_per_cycle) + 1) / self.iter_per_cycle\n",
    "\n",
    "    def _t_epoch(self):\n",
    "        return (((self.clr_iterations - 1) % self.iter_per_epoch) + 1) / self.iter_per_epoch\n",
    "\n",
    "    def snapshot_predict(self):\n",
    "        for seq_name, seq in self.seqs_dict.items():\n",
    "            self.probs_dict[seq_name].append(self.model.predict_generator(seq, steps=len(seq),\n",
    "                                                                          use_multiprocessing=False, workers=CPU_CORES,\n",
    "                                                                          max_queue_size=2 * CPU_CORES + 2,\n",
    "                                                                          verbose=0))\n",
    "\n",
    "    def swa_weights_update(self):\n",
    "        weights = self.model.get_weights()\n",
    "\n",
    "        if len(self.swa_weights) == 0:\n",
    "            self.swa_weights = weights\n",
    "            return\n",
    "\n",
    "        for i in range(0, len(self.swa_weights)):\n",
    "            self.swa_weights[i] = (self.swa_weights[i] * self.model_counts + weights[i]) / (self.model_counts + 1)\n",
    "train_ids_list = train_ids.tolist()\n",
    "instids = train2.loc[(train2.metric_point==1) & (train2.installation_id.isin(train_ids_list)),'installation_id'].values\n",
    "train_ids_list = train2.loc[(train2.metric_point==1)&(train2.installation_id.isin(train_ids_list))].installation_id.unique().tolist()\n",
    "print(len(train_ids_list))\n",
    "\n",
    "instids_unique = list(train_ids_list)\n",
    "instids = train2.loc[(train2.metric_point==1),'installation_id'].values\n",
    "\n",
    "instids2_unique = train2.installation_id.unique()\n",
    "\n",
    "seqlens = []\n",
    "for qid in tqdm_notebook(train_ids_list):\n",
    "    seqlens.append(train2.loc[(train2.metric_point==1)&(train2.installation_id==qid),'metric_point'].sum())\n",
    "print(len(seqlens), q_train.shape)\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "train_ids_list = train2.loc[(train2.metric_point==1)&(train2.installation_id.isin(train_ids_list))].installation_id.unique()\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "NFOLDS = 7\n",
    "folds1 = KFold(n_splits=NFOLDS, shuffle=True, random_state=2019)\n",
    "folds2 = KFold(n_splits=NFOLDS, shuffle=True, random_state=239)\n",
    "folds3 = KFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "instids = train2.loc[(train2.metric_point==1),'installation_id'].values\n",
    "\n",
    "seq_len = 64\n",
    "#num_cols = ['correct_attempts', 'incorrect_attempts', 'time_df', 'event_duration', 'code_4070_count',  'code_2010_count']\n",
    "num_cols = list(range(3+4+len(code_counts_cols)))\n",
    "\n",
    "def get_cat_emb(cat_name, cat_size, min_emb_size=2, max_emb_size=50, reg=regularizers.l2(3e-4)): # regularizers.l2(1e-4)\n",
    "    emb_size = 7\n",
    "    emb_inp = Input((seq_len,), name=cat_name+'_in')\n",
    "    #emb = Dropout(0.005)(emb_inp)\n",
    "    emb = Embedding(cat_size, emb_size, name=cat_name+'_emb', mask_zero=True)(emb_inp) #, embeddings_regularizer=reg\n",
    "    return emb_inp, emb\n",
    "    \n",
    "def buildMixedModel(cat_cols, cat_sizes_map):\n",
    "    cat_inps = []\n",
    "    cat_embs = []\n",
    "    for cat_col in cat_cols:\n",
    "        emb_inp, emb = get_cat_emb(cat_col, cat_sizes_map[cat_col])\n",
    "        cat_inps.append(emb_inp)\n",
    "        cat_embs.append(emb)\n",
    "    \n",
    "    num_inp = Input((seq_len,len(num_cols)), name='num_inp')\n",
    "    text_inp = Input((q_train.shape[1],), name='text_inp')\n",
    "    x2 = text_inp\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "    x2 = Dense(96, activation='relu')(x2)\n",
    "\n",
    "    cat_embs.append(num_inp)\n",
    "    x = Concatenate(axis=-1)(cat_embs)\n",
    "    \n",
    "    x = Bidirectional(GRU(64, return_sequences=True, recurrent_dropout=0.001, activation='relu'))(x)\n",
    "    #x = Flatten()(x)\n",
    "    #x = Dropout(0.5)(x)\n",
    "    #x = LSTM(64, return_sequences=True, recurrent_dropout=0.001, activation='relu')(x)\n",
    "    #x = Dropout(0.5)(x)\n",
    "    #x = LSTM(80, return_sequences=True, recurrent_dropout=0.001, activation='relu')(x)\n",
    "    #x = Flatten()(x)\n",
    "    x = AttentionWeightedAverage()(x)\n",
    "    x = Concatenate()([x,x2])\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    output1 = Dense(1, activation=\"linear\", name=\"output1\")(x)\n",
    "\n",
    "    return Model(inputs=cat_inps + [num_inp,text_inp], outputs=[output1])\n",
    "model = buildMixedModel(cat_cols, cat_sizes_map)\n",
    "model.summary()\n",
    "\n",
    "test_pred = np.zeros(matrix_titles_test.shape[0])\n",
    "raw_preds = np.zeros((matrix_titles_test.shape[0], 15))\n",
    "\n",
    "histories = []\n",
    "metrics_last_assessment_val = []\n",
    "metrics_val = []\n",
    "\n",
    "def get_inps(idxs):\n",
    "    inps = []\n",
    "    inps.append(matrix_titles[idxs, -seq_len:])\n",
    "    inps.append(matrix_numericals[idxs, -seq_len:, :][:,:,num_cols])\n",
    "    inps.append(q_train[idxs].todense())\n",
    "    return inps\n",
    "\n",
    "test_seq = FeatureSequence([matrix_titles_test[:, -seq_len:],\n",
    "                            matrix_numericals_test[:, -seq_len:, :][:,:,num_cols], \n",
    "                            q_test.todense()], None, 128, shuffle=False)\n",
    "\n",
    "\n",
    "def random_chs(a):\n",
    "    size = 1\n",
    "    replace = True\n",
    "    fn = lambda obj: obj.loc[np.random.choice(obj.index, size, replace),:]\n",
    "    df = pd.DataFrame({'Group_Id':a})\n",
    "    q = df.groupby('Group_Id', as_index=False).apply(fn).reset_index()\n",
    "    q.columns = ['v0','v1','v2']\n",
    "    return q.v1.values.astype(int)\n",
    "\n",
    "ifold = 0\n",
    "gcoefs = [0,0,0]\n",
    "\n",
    "for folds in [folds1]:\n",
    "    for fold, (train_idxs, val_idxs) in enumerate(folds.split(instids)):\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "        gc.collect()    \n",
    "\n",
    "        smpls = np.array([np.arange(len(val_idxs))])\n",
    "\n",
    "        trn_seq = FeatureSequence(get_inps(train_idxs),\n",
    "                                  [train2.loc[(train2.metric_point == 1),'label'].values[train_idxs]], \n",
    "                                  128, shuffle=True)\n",
    "        val_seq = FeatureSequence(get_inps(val_idxs),\n",
    "                                  [train2.loc[(train2.metric_point == 1),'label'].values[val_idxs]], \n",
    "                                  len(val_idxs), shuffle=False)\n",
    "\n",
    "        Y = train2.loc[(train2.metric_point == 1),'label'].values[val_idxs]\n",
    "        Y2 = train2.loc[(train2.metric_point == 1),'all_attempts'].values[val_idxs]\n",
    "        kappa_metric = KappaEvaluationSeq(val_seq, Y, Y2, smpls, 'val')\n",
    "\n",
    "        model_file = 'model_' + str(fold) + '.pth'\n",
    "        early_stop = EarlyStopping(monitor='val_kappa', min_delta=0, patience=16, verbose=1, mode='max')\n",
    "        model_checkpoint = ModelCheckpoint(model_file, monitor='val_kappa', verbose=1, mode='max',\n",
    "                                           save_best_only=True, save_weights_only=False, period=1)\n",
    "        clr = CyclicLR(base_lr=0.00001, max_lr=0.001, step_size=4*math.ceil(len(trn_seq)), mode='triangular2')\n",
    "        model = buildMixedModel(cat_cols, cat_sizes_map)\n",
    "        opt=keras.optimizers.Adam(lr=0.001, clipnorm=1.0, clipvalue=1.0)\n",
    "        #opt = AdamW(weight_decay=0.015, beta_1=0.9, beta_2=0.999, batch_size=8,\n",
    "        #            samples_per_epoch=len(train_idxs), epochs=8,\n",
    "        #            clipnorm=0, clipvalue=0)\n",
    "\n",
    "        model.compile(optimizer=opt, loss=['mean_squared_error'])\n",
    "\n",
    "        seqs_dict = {\"val\": val_seq, \"test\": test_seq}\n",
    "\n",
    "        se = StochasticEnsembling(seqs_dict=seqs_dict, cycle_len=12, iter_per_epoch=len(trn_seq),\n",
    "                                  alpha1=0.005, alpha2=0.0005, lr_schedule_mode=\"clr\",\n",
    "                                  swa_cycle_start_inx=0, model_name=\"model\", verbose=1)\n",
    "\n",
    "        history = model.fit_generator(\n",
    "                generator=trn_seq, steps_per_epoch=len(trn_seq),\n",
    "                initial_epoch=0, epochs=24, shuffle=False, verbose=2,\n",
    "                callbacks=[kappa_metric, se],  #clr, model_checkpoint, early_stop\n",
    "                use_multiprocessing=False, workers=1, max_queue_size=2*4)\n",
    "\n",
    "        histories.append(history)\n",
    "\n",
    "        #model = load_model(model_file, custom_objects = {\n",
    "        #    'AttentionWeightedAverage': AttentionWeightedAverage\n",
    "        #})\n",
    "\n",
    "        qpred = (se.probs_dict[\"val\"][:,-2] + se.probs_dict[\"val\"][:,-3])/2.0\n",
    "\n",
    "        optR = OptimizedRounder(smpls) \n",
    "        optR.fit(qpred, Y)\n",
    "        coefficients = optR.coefficients()\n",
    "        for ic in range(3):\n",
    "            gcoefs[ic] += coefficients[ic] / NFOLDS\n",
    "        y_pred = optR.predict(qpred, coefficients)     \n",
    "\n",
    "        tts = []\n",
    "        for i in range(smpls.shape[0]):\n",
    "            tts.append(qwk3(np.array(Y)[smpls[i].astype(int)], np.array(y_pred.astype(int))[smpls[i].astype(int)]))\n",
    "        kapa = np.median(tts)     \n",
    "\n",
    "        pred1 = (se.probs_dict[\"test\"][:,-2] + se.probs_dict[\"test\"][:,-3])/2.0\n",
    "        \n",
    "        test_pred += pred1.ravel() / NFOLDS\n",
    "        \n",
    "        raw_preds[:,ifold] = pred1.ravel()\n",
    "        ifold += 1\n",
    "        \n",
    "        pred1 = optR.predict(pred1, coefficients).ravel().astype(int)\n",
    "\n",
    "        del model, kappa_metric, clr, model_checkpoint, early_stop, se\n",
    "        del trn_seq, val_seq, qpred, Y, y_pred\n",
    "        print(fold, kapa)\n",
    "        gc.collect()\n",
    "\n",
    "        metrics_val.append(kapa)\n",
    "\n",
    "np.save('v24full', raw_preds)\n",
    "\n",
    "print(np.mean(metrics_val))\n",
    "\n",
    "optR = OptimizedRounder([1])\n",
    "y_pred = optR.predict(test_pred, gcoefs).ravel()\n",
    "\n",
    "submission = pd.DataFrame({'installation_id':test_ids})\n",
    "submission['accuracy_group'] = y_pred.astype('int')\n",
    "submission.to_csv('submission.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\r\n",
      "(303319, 68) (28445, 68) (19708, 7492) (1000, 7492) (17690, 68) (2018, 68)\r\n",
      "Process categorical features:\r\n",
      "incorrect_attempts 13.0\r\n",
      "all_attempts 13.0\r\n",
      "game_duration 12.652870724256527\r\n",
      "BirdMeasurer(Assessment)_correct 2.0\r\n",
      "BirdMeasurer(Assessment)_incorrect 19.0\r\n",
      "BirdMeasurer(Assessment)_rate 0.6666666666666666\r\n",
      "CartBalancer(Assessment)_correct 5.0\r\n",
      "CartBalancer(Assessment)_incorrect 7.0\r\n",
      "CartBalancer(Assessment)_rate 1.0\r\n",
      "CauldronFiller(Assessment)_correct 6.0\r\n",
      "CauldronFiller(Assessment)_incorrect 10.0\r\n",
      "CauldronFiller(Assessment)_rate 1.0\r\n",
      "ChestSorter(Assessment)_correct 2.0\r\n",
      "ChestSorter(Assessment)_incorrect 13.0\r\n",
      "ChestSorter(Assessment)_rate 0.5\r\n",
      "MushroomSorter(Assessment)_correct 5.0\r\n",
      "MushroomSorter(Assessment)_incorrect 8.930000000000291\r\n",
      "MushroomSorter(Assessment)_rate 1.0\r\n",
      "event_code_3010 9.0\r\n",
      "event_code_3110 9.0\r\n",
      "event_code_4020 22.0\r\n",
      "event_code_4021 0.0\r\n",
      "event_code_4030 47.0\r\n",
      "event_code_4035 6.0\r\n",
      "event_code_4090 1.930000000000291\r\n",
      "event_code_2020 2.0\r\n",
      "event_code_2030 2.0\r\n",
      "event_code_2040 0.0\r\n",
      "event_code_2050 0.0\r\n",
      "event_code_2080 0.0\r\n",
      "event_code_2083 0.0\r\n",
      "event_code_3021 4.0\r\n",
      "event_code_3120 14.0\r\n",
      "event_code_4010 0.0\r\n",
      "event_code_2060 0.0\r\n",
      "event_code_2070 0.0\r\n",
      "event_code_4031 0.0\r\n",
      "event_code_4025 27.0\r\n",
      "event_code_5000 0.0\r\n",
      "event_code_5010 0.0\r\n",
      "event_code_2081 0.0\r\n",
      "event_code_2025 1.0\r\n",
      "event_code_4022 0.0\r\n",
      "event_code_2010 1.0\r\n",
      "event_code_2035 1.0\r\n",
      "event_code_4040 11.0\r\n",
      "event_code_4100 10.0\r\n",
      "event_code_4110 11.0\r\n",
      "event_code_4045 0.0\r\n",
      "event_code_4095 0.0\r\n",
      "event_code_4220 0.0\r\n",
      "event_code_2075 0.0\r\n",
      "event_code_4230 0.0\r\n",
      "event_code_4235 0.0\r\n",
      "event_code_4080 0.0\r\n",
      "event_code_4050 0.0\r\n",
      "{'incorrect_attempts': 13.0, 'all_attempts': 13.0, 'game_duration': 12.652870724256527, 'BirdMeasurer(Assessment)_correct': 2.0, 'BirdMeasurer(Assessment)_incorrect': 19.0, 'BirdMeasurer(Assessment)_rate': 0.6666666666666666, 'CartBalancer(Assessment)_correct': 5.0, 'CartBalancer(Assessment)_incorrect': 7.0, 'CartBalancer(Assessment)_rate': 1.0, 'CauldronFiller(Assessment)_correct': 6.0, 'CauldronFiller(Assessment)_incorrect': 10.0, 'CauldronFiller(Assessment)_rate': 1.0, 'ChestSorter(Assessment)_correct': 2.0, 'ChestSorter(Assessment)_incorrect': 13.0, 'ChestSorter(Assessment)_rate': 0.5, 'MushroomSorter(Assessment)_correct': 5.0, 'MushroomSorter(Assessment)_incorrect': 8.930000000000291, 'MushroomSorter(Assessment)_rate': 1.0, 'event_code_3010': 9.0, 'event_code_3110': 9.0, 'event_code_4020': 22.0, 'event_code_4021': 0.0, 'event_code_4030': 47.0, 'event_code_4035': 6.0, 'event_code_4090': 1.930000000000291, 'event_code_2020': 2.0, 'event_code_2030': 2.0, 'event_code_2040': 0.0, 'event_code_2050': 0.0, 'event_code_2080': 0.0, 'event_code_2083': 0.0, 'event_code_3021': 4.0, 'event_code_3120': 14.0, 'event_code_4010': 0.0, 'event_code_2060': 0.0, 'event_code_2070': 0.0, 'event_code_4031': 0.0, 'event_code_4025': 27.0, 'event_code_5000': 0.0, 'event_code_5010': 0.0, 'event_code_2081': 0.0, 'event_code_2025': 1.0, 'event_code_4022': 0.0, 'event_code_2010': 1.0, 'event_code_2035': 1.0, 'event_code_4040': 11.0, 'event_code_4100': 10.0, 'event_code_4110': 11.0, 'event_code_4045': 0.0, 'event_code_4095': 0.0, 'event_code_4220': 0.0, 'event_code_2075': 0.0, 'event_code_4230': 0.0, 'event_code_4235': 0.0, 'event_code_4080': 0.0, 'event_code_4050': 0.0}\r\n",
      "HBox(children=(IntProgress(value=0, max=19708), HTML(value='')))\r\n",
      "\r\n",
      "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))\r\n",
      "\r\n",
      "(1000, 64, 45) (1000, 64)\r\n",
      "3614\r\n",
      "HBox(children=(IntProgress(value=0, max=3614), HTML(value='')))\r\n",
      "\r\n",
      "3614 (19708, 7492)\r\n",
      "2020-01-10 07:48:16.396763: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\r\n",
      "2020-01-10 07:48:16.397228: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55de91b99b20 executing computations on platform Host. Devices:\r\n",
      "2020-01-10 07:48:16.397276: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n",
      "Model: \"model_1\"\r\n",
      "__________________________________________________________________________________________________\r\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \r\n",
      "==================================================================================================\r\n",
      "title_in (InputLayer)           (None, 64)           0                                            \r\n",
      "__________________________________________________________________________________________________\r\n",
      "title_emb (Embedding)           (None, 64, 7)        315         title_in[0][0]                   \r\n",
      "__________________________________________________________________________________________________\r\n",
      "num_inp (InputLayer)            (None, 64, 45)       0                                            \r\n",
      "__________________________________________________________________________________________________\r\n",
      "concatenate_1 (Concatenate)     (None, 64, 52)       0           title_emb[0][0]                  \r\n",
      "                                                                 num_inp[0][0]                    \r\n",
      "__________________________________________________________________________________________________\r\n",
      "text_inp (InputLayer)           (None, 7492)         0                                            \r\n",
      "__________________________________________________________________________________________________\r\n",
      "bidirectional_1 (Bidirectional) (None, 64, 128)      44928       concatenate_1[0][0]              \r\n",
      "__________________________________________________________________________________________________\r\n",
      "dropout_1 (Dropout)             (None, 7492)         0           text_inp[0][0]                   \r\n",
      "__________________________________________________________________________________________________\r\n",
      "attention_weighted_average_1 (A (None, 128)          128         bidirectional_1[0][0]            \r\n",
      "__________________________________________________________________________________________________\r\n",
      "dense_1 (Dense)                 (None, 96)           719328      dropout_1[0][0]                  \r\n",
      "__________________________________________________________________________________________________\r\n",
      "concatenate_2 (Concatenate)     (None, 224)          0           attention_weighted_average_1[0][0\r\n",
      "                                                                 dense_1[0][0]                    \r\n",
      "__________________________________________________________________________________________________\r\n",
      "dropout_2 (Dropout)             (None, 224)          0           concatenate_2[0][0]              \r\n",
      "__________________________________________________________________________________________________\r\n",
      "dense_2 (Dense)                 (None, 256)          57600       dropout_2[0][0]                  \r\n",
      "__________________________________________________________________________________________________\r\n",
      "dropout_3 (Dropout)             (None, 256)          0           dense_2[0][0]                    \r\n",
      "__________________________________________________________________________________________________\r\n",
      "dense_3 (Dense)                 (None, 128)          32896       dropout_3[0][0]                  \r\n",
      "__________________________________________________________________________________________________\r\n",
      "dropout_4 (Dropout)             (None, 128)          0           dense_3[0][0]                    \r\n",
      "__________________________________________________________________________________________________\r\n",
      "dense_4 (Dense)                 (None, 64)           8256        dropout_4[0][0]                  \r\n",
      "__________________________________________________________________________________________________\r\n",
      "dropout_5 (Dropout)             (None, 64)           0           dense_4[0][0]                    \r\n",
      "__________________________________________________________________________________________________\r\n",
      "output1 (Dense)                 (None, 1)            65          dropout_5[0][0]                  \r\n",
      "==================================================================================================\r\n",
      "Total params: 863,516\r\n",
      "Trainable params: 863,516\r\n",
      "Non-trainable params: 0\r\n",
      "__________________________________________________________________________________________________\r\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\r\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\r\n",
      "Epoch 1/24\r\n",
      " - 23s - loss: 2.0725\r\n",
      "train_model.py:207: NumbaWarning: \u001b[1m\r\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"qwk3\" failed type inference due to: \u001b[1m\u001b[1m\u001b[1mInvalid use of Function(<function asarray at 0x7ffaa15caa60>) with argument(s) of type(s): (array(int64, 1d, C), dtype=Function(<class 'int'>))\r\n",
      " * parameterized\r\n",
      "\u001b[1mIn definition 0:\u001b[0m\r\n",
      "\u001b[1m    AttributeError: 'Function' object has no attribute 'dtype'\u001b[0m\r\n",
      "    raised from /opt/conda/lib/python3.6/site-packages/numba/targets/arraymath.py:3679\r\n",
      "\u001b[1mIn definition 1:\u001b[0m\r\n",
      "\u001b[1m    AttributeError: 'Function' object has no attribute 'dtype'\u001b[0m\r\n",
      "    raised from /opt/conda/lib/python3.6/site-packages/numba/targets/arraymath.py:3679\r\n",
      "\u001b[1mThis error is usually caused by passing an argument of a type that is unsupported by the named function.\u001b[0m\u001b[0m\r\n",
      "\u001b[0m\u001b[1m[1] During: resolving callee type: Function(<function asarray at 0x7ffaa15caa60>)\u001b[0m\r\n",
      "\u001b[0m\u001b[1m[2] During: typing of call at train_model.py (210)\r\n",
      "\u001b[0m\r\n",
      "\u001b[1m\r\n",
      "File \"train_model.py\", line 210:\u001b[0m\r\n",
      "\u001b[1mdef qwk3(a1, a2):\r\n",
      "    <source elided>\r\n",
      "    assert(len(a1) == len(a2))\r\n",
      "\u001b[1m    a1 = np.asarray(a1, dtype=int)\r\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\r\n",
      "\u001b[0m\r\n",
      "  @jit\r\n",
      "train_model.py:207: NumbaWarning: \u001b[1m\r\n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"qwk3\" failed type inference due to: \u001b[1mUnsupported constraint encountered: raise $16.1\r\n",
      "\u001b[1m\r\n",
      "File \"train_model.py\", line 209:\u001b[0m\r\n",
      "\u001b[1mdef qwk3(a1, a2):\r\n",
      "\u001b[1m    assert(len(a1) == len(a2))\r\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\r\n",
      "\u001b[0m\u001b[0m\r\n",
      "  @jit\r\n",
      "/opt/conda/lib/python3.6/site-packages/numba/compiler.py:742: NumbaWarning: \u001b[1mFunction \"qwk3\" was compiled in object mode without forceobj=True, but has lifted loops.\r\n",
      "\u001b[1m\r\n",
      "File \"train_model.py\", line 208:\u001b[0m\r\n",
      "\u001b[1m@jit\r\n",
      "\u001b[1mdef qwk3(a1, a2):\r\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\r\n",
      "\u001b[0m\r\n",
      "  self.func_ir.loc))\r\n",
      "/opt/conda/lib/python3.6/site-packages/numba/compiler.py:751: NumbaDeprecationWarning: \u001b[1m\r\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\r\n",
      "\r\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\r\n",
      "\u001b[1m\r\n",
      "File \"train_model.py\", line 208:\u001b[0m\r\n",
      "\u001b[1m@jit\r\n",
      "\u001b[1mdef qwk3(a1, a2):\r\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\r\n",
      "\u001b[0m\r\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc))\r\n",
      "train_model.py:207: NumbaWarning: \u001b[1m\r\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"qwk3\" failed type inference due to: \u001b[1m\u001b[1m\u001b[1mInvalid use of Function(<function asarray at 0x7ffaa15caa60>) with argument(s) of type(s): (array(int64, 1d, C), dtype=Function(<class 'int'>))\r\n",
      " * parameterized\r\n",
      "\u001b[1mIn definition 0:\u001b[0m\r\n",
      "\u001b[1m    AttributeError: 'Function' object has no attribute 'dtype'\u001b[0m\r\n",
      "    raised from /opt/conda/lib/python3.6/site-packages/numba/targets/arraymath.py:3679\r\n",
      "\u001b[1mIn definition 1:\u001b[0m\r\n",
      "\u001b[1m    AttributeError: 'Function' object has no attribute 'dtype'\u001b[0m\r\n",
      "    raised from /opt/conda/lib/python3.6/site-packages/numba/targets/arraymath.py:3679\r\n",
      "\u001b[1mThis error is usually caused by passing an argument of a type that is unsupported by the named function.\u001b[0m\u001b[0m\r\n",
      "\u001b[0m\u001b[1m[1] During: resolving callee type: Function(<function asarray at 0x7ffaa15caa60>)\u001b[0m\r\n",
      "\u001b[0m\u001b[1m[2] During: typing of call at train_model.py (210)\r\n",
      "\u001b[0m\r\n",
      "\u001b[1m\r\n",
      "File \"train_model.py\", line 210:\u001b[0m\r\n",
      "\u001b[1mdef qwk3(a1, a2):\r\n",
      "    <source elided>\r\n",
      "    assert(len(a1) == len(a2))\r\n",
      "\u001b[1m    a1 = np.asarray(a1, dtype=int)\r\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\r\n",
      "\u001b[0m\r\n",
      "  @jit\r\n",
      "train_model.py:207: NumbaWarning: \u001b[1m\r\n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"qwk3\" failed type inference due to: \u001b[1mUnsupported constraint encountered: raise $16.1\r\n",
      "\u001b[1m\r\n",
      "File \"train_model.py\", line 209:\u001b[0m\r\n",
      "\u001b[1mdef qwk3(a1, a2):\r\n",
      "\u001b[1m    assert(len(a1) == len(a2))\r\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\r\n",
      "\u001b[0m\u001b[0m\r\n",
      "  @jit\r\n",
      "/opt/conda/lib/python3.6/site-packages/numba/compiler.py:742: NumbaWarning: \u001b[1mFunction \"qwk3\" was compiled in object mode without forceobj=True, but has lifted loops.\r\n",
      "\u001b[1m\r\n",
      "File \"train_model.py\", line 208:\u001b[0m\r\n",
      "\u001b[1m@jit\r\n",
      "\u001b[1mdef qwk3(a1, a2):\r\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\r\n",
      "\u001b[0m\r\n",
      "  self.func_ir.loc))\r\n",
      "/opt/conda/lib/python3.6/site-packages/numba/compiler.py:751: NumbaDeprecationWarning: \u001b[1m\r\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\r\n",
      "\r\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\r\n",
      "\u001b[1m\r\n",
      "File \"train_model.py\", line 208:\u001b[0m\r\n",
      "\u001b[1m@jit\r\n",
      "\u001b[1mdef qwk3(a1, a2):\r\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\r\n",
      "\u001b[0m\r\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc))\r\n",
      "val_kappa: 0.4980; val_mae: 0.9854; [1.3258,1.4419,1.7233]\r\n",
      "Epoch 2/24\r\n",
      " - 21s - loss: 1.2971\r\n",
      "val_kappa: 0.5677; val_mae: 0.9734; [1.3581,1.5071,1.8267]\r\n",
      "Epoch 3/24\r\n",
      " - 21s - loss: 1.1857\r\n",
      "val_kappa: 0.5628; val_mae: 0.8635; [1.1562,1.7383,2.1668]\r\n",
      "Epoch 4/24\r\n",
      " - 21s - loss: 1.1208\r\n",
      "val_kappa: 0.5796; val_mae: 0.9015; [1.1139,1.7495,2.0548]\r\n",
      "Epoch 5/24\r\n",
      " - 21s - loss: 1.0923\r\n",
      "val_kappa: 0.5783; val_mae: 0.8870; [1.1101,1.8200,2.1484]\r\n",
      "Epoch 6/24\r\n",
      " - 21s - loss: 1.0636\r\n",
      "val_kappa: 0.5509; val_mae: 0.8932; [1.1806,1.6498,2.1498]\r\n",
      "Epoch 7/24\r\n",
      " - 21s - loss: 1.0670\r\n",
      "val_kappa: 0.5868; val_mae: 0.8671; [1.0545,1.5773,2.1269]\r\n",
      "Epoch 8/24\r\n",
      " - 21s - loss: 1.0217\r\n",
      "val_kappa: 0.5698; val_mae: 0.8783; [1.0942,1.5543,2.1555]\r\n",
      "Epoch 9/24\r\n",
      " - 21s - loss: 1.0104\r\n",
      "val_kappa: 0.5798; val_mae: 0.8627; [1.1400,1.7674,2.2804]\r\n",
      "Epoch 10/24\r\n",
      " - 21s - loss: 0.9869\r\n",
      "val_kappa: 0.5864; val_mae: 0.8530; [1.1894,1.5096,2.3087]\r\n",
      "Epoch 11/24\r\n",
      " - 22s - loss: 0.9688\r\n",
      "val_kappa: 0.5889; val_mae: 0.8440; [1.1099,1.8386,2.2966]\r\n",
      "Epoch 12/24\r\n",
      " - 21s - loss: 0.9518\r\n",
      "val_kappa: 0.5930; val_mae: 0.8408; [1.1361,1.5543,2.3470]\r\n",
      "Latest lr: 0.00050\r\n",
      "Reached 1 cycle\r\n",
      "Epoch 13/24\r\n",
      " - 21s - loss: 0.9409\r\n",
      "val_kappa: 0.5906; val_mae: 0.8240; [1.2287,1.6021,2.3130]\r\n",
      "Epoch 14/24\r\n",
      " - 21s - loss: 0.9434\r\n",
      "val_kappa: 0.5951; val_mae: 0.8499; [1.1197,1.7223,2.3432]\r\n",
      "Epoch 15/24\r\n",
      " - 22s - loss: 0.9533\r\n",
      "val_kappa: 0.5887; val_mae: 0.8348; [1.1612,1.7272,2.2291]\r\n",
      "Epoch 16/24\r\n",
      " - 21s - loss: 0.9561\r\n",
      "val_kappa: 0.5843; val_mae: 0.8377; [1.0658,1.8085,2.4001]\r\n",
      "Epoch 17/24\r\n",
      " - 21s - loss: 0.9700\r\n",
      "val_kappa: 0.5757; val_mae: 0.8374; [1.1058,1.5956,2.4174]\r\n",
      "Epoch 18/24\r\n",
      " - 21s - loss: 0.9715\r\n",
      "val_kappa: 0.5892; val_mae: 0.8491; [1.1604,1.6606,2.2547]\r\n",
      "Epoch 19/24\r\n",
      " - 21s - loss: 0.9654\r\n",
      "val_kappa: 0.5856; val_mae: 0.8402; [1.1383,1.8183,2.1348]\r\n",
      "Epoch 20/24\r\n",
      " - 21s - loss: 0.9491\r\n",
      "val_kappa: 0.5840; val_mae: 0.8503; [1.1184,1.6891,2.2906]\r\n",
      "Epoch 21/24\r\n",
      " - 21s - loss: 0.9451\r\n",
      "val_kappa: 0.5898; val_mae: 0.8329; [1.1338,1.6588,2.3302]\r\n",
      "Epoch 22/24\r\n",
      " - 21s - loss: 0.9214\r\n",
      "val_kappa: 0.5935; val_mae: 0.8351; [1.1681,1.6526,2.3271]\r\n",
      "Epoch 23/24\r\n",
      " - 22s - loss: 0.9051\r\n",
      "val_kappa: 0.5962; val_mae: 0.8271; [1.0881,1.7618,2.2970]\r\n",
      "Epoch 24/24\r\n",
      " - 22s - loss: 0.8800\r\n",
      "val_kappa: 0.5982; val_mae: 0.8239; [0.9970,1.8131,2.3145]\r\n",
      "Latest lr: 0.00050\r\n",
      "Reached 2 cycle\r\n",
      "0 0.5968657269503161\r\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\r\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\r\n",
      "Epoch 1/24\r\n",
      " - 23s - loss: 1.8197\r\n",
      "val_kappa: 0.3837; val_mae: 1.1357; [1.1260,1.1555,2.6900]\r\n",
      "Epoch 2/24\r\n",
      " - 21s - loss: 1.3100\r\n",
      "val_kappa: 0.5839; val_mae: 0.9607; [1.1811,1.4049,1.7411]\r\n",
      "Epoch 3/24\r\n",
      " - 21s - loss: 1.1816\r\n",
      "val_kappa: 0.5974; val_mae: 0.8730; [1.2680,1.7561,1.9663]\r\n",
      "Epoch 4/24\r\n",
      " - 22s - loss: 1.1139\r\n",
      "val_kappa: 0.5873; val_mae: 0.8566; [1.2210,1.7549,2.0022]\r\n",
      "Epoch 5/24\r\n",
      " - 21s - loss: 1.0840\r\n",
      "val_kappa: 0.5921; val_mae: 0.8252; [1.0788,1.7783,2.2681]\r\n",
      "Epoch 6/24\r\n",
      " - 21s - loss: 1.0784\r\n",
      "val_kappa: 0.5893; val_mae: 0.8934; [0.7349,1.5044,2.1190]\r\n",
      "Epoch 7/24\r\n",
      " - 21s - loss: 1.0617\r\n",
      "val_kappa: 0.5922; val_mae: 0.8337; [1.1511,1.6875,2.3077]\r\n",
      "Epoch 8/24\r\n",
      " - 22s - loss: 1.0399\r\n",
      "val_kappa: 0.6033; val_mae: 0.8523; [1.0873,1.7234,2.2352]\r\n",
      "Epoch 9/24\r\n",
      " - 21s - loss: 1.0043\r\n",
      "val_kappa: 0.6052; val_mae: 0.8369; [1.1897,1.5581,2.3324]\r\n",
      "Epoch 10/24\r\n",
      " - 22s - loss: 0.9876\r\n",
      "val_kappa: 0.5992; val_mae: 0.8357; [1.0660,1.7803,2.3008]\r\n",
      "Epoch 11/24\r\n",
      " - 21s - loss: 0.9632\r\n",
      "val_kappa: 0.6100; val_mae: 0.8366; [1.1473,1.7396,2.2965]\r\n",
      "Epoch 12/24\r\n",
      " - 21s - loss: 0.9547\r\n",
      "val_kappa: 0.6093; val_mae: 0.8300; [1.1925,1.6212,2.3820]\r\n",
      "Latest lr: 0.00050\r\n",
      "Reached 1 cycle\r\n",
      "Epoch 13/24\r\n",
      " - 21s - loss: 0.9385\r\n",
      "val_kappa: 0.6094; val_mae: 0.8309; [1.1772,1.7402,2.2288]\r\n",
      "Epoch 14/24\r\n",
      " - 21s - loss: 0.9458\r\n",
      "val_kappa: 0.6178; val_mae: 0.8166; [1.1162,1.6416,2.3618]\r\n",
      "Epoch 15/24\r\n",
      " - 21s - loss: 0.9442\r\n",
      "val_kappa: 0.6073; val_mae: 0.8395; [1.1444,1.3527,2.1247]\r\n",
      "Epoch 16/24\r\n",
      " - 21s - loss: 0.9447\r\n",
      "val_kappa: 0.6129; val_mae: 0.8319; [1.0477,1.7282,2.3355]\r\n",
      "Epoch 17/24\r\n",
      " - 21s - loss: 0.9667\r\n",
      "val_kappa: 0.6075; val_mae: 0.8257; [1.0045,1.7995,2.3705]\r\n",
      "Epoch 18/24\r\n",
      " - 21s - loss: 0.9666\r\n",
      "val_kappa: 0.6146; val_mae: 0.7921; [1.0029,1.6096,2.5504]\r\n",
      "Epoch 19/24\r\n",
      " - 21s - loss: 0.9738\r\n",
      "val_kappa: 0.6078; val_mae: 0.8298; [1.1333,1.5465,2.3568]\r\n",
      "Epoch 20/24\r\n",
      " - 21s - loss: 0.9623\r\n",
      "val_kappa: 0.6132; val_mae: 0.8151; [1.0031,1.7215,2.3812]\r\n",
      "Epoch 21/24\r\n",
      " - 21s - loss: 0.9381\r\n",
      "val_kappa: 0.6112; val_mae: 0.8036; [0.9337,1.5536,2.4474]\r\n",
      "Epoch 22/24\r\n",
      " - 21s - loss: 0.9219\r\n",
      "val_kappa: 0.6116; val_mae: 0.8158; [1.1118,1.6954,2.3731]\r\n",
      "Epoch 23/24\r\n",
      " - 21s - loss: 0.9057\r\n",
      "val_kappa: 0.6142; val_mae: 0.8184; [1.1697,1.6913,2.4360]\r\n",
      "Epoch 24/24\r\n",
      " - 21s - loss: 0.9005\r\n",
      "val_kappa: 0.6088; val_mae: 0.8184; [1.0467,1.6313,2.4396]\r\n",
      "Latest lr: 0.00050\r\n",
      "Reached 2 cycle\r\n",
      "1 0.6156766181160118\r\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\r\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\r\n",
      "Epoch 1/24\r\n",
      " - 24s - loss: 1.8977\r\n",
      "val_kappa: 0.4147; val_mae: 1.1006; [1.2076,1.2373,2.4539]\r\n",
      "Epoch 2/24\r\n",
      " - 22s - loss: 1.2875\r\n",
      "val_kappa: 0.5769; val_mae: 0.9269; [1.2185,1.6080,1.7849]\r\n",
      "Epoch 3/24\r\n",
      " - 23s - loss: 1.1907\r\n",
      "val_kappa: 0.5639; val_mae: 0.8815; [1.1511,1.8152,2.1183]\r\n",
      "Epoch 4/24\r\n",
      " - 21s - loss: 1.1279\r\n",
      "val_kappa: 0.5840; val_mae: 0.9043; [1.1592,1.7251,2.0604]\r\n",
      "Epoch 5/24\r\n",
      " - 21s - loss: 1.0853\r\n",
      "val_kappa: 0.5953; val_mae: 0.8610; [1.2182,1.8459,2.2598]\r\n",
      "Epoch 6/24\r\n",
      " - 21s - loss: 1.0732\r\n",
      "val_kappa: 0.5829; val_mae: 0.8374; [1.1505,1.8243,2.2334]\r\n",
      "Epoch 7/24\r\n",
      " - 21s - loss: 1.0531\r\n",
      "val_kappa: 0.5899; val_mae: 0.8412; [1.1377,1.7292,2.2265]\r\n",
      "Epoch 8/24\r\n",
      " - 22s - loss: 1.0299\r\n",
      "val_kappa: 0.5975; val_mae: 0.8464; [1.1453,1.7111,2.2935]\r\n",
      "Epoch 9/24\r\n",
      " - 21s - loss: 1.0049\r\n",
      "val_kappa: 0.5963; val_mae: 0.8529; [1.1948,1.6586,2.3067]\r\n",
      "Epoch 10/24\r\n",
      " - 21s - loss: 0.9805\r\n",
      "val_kappa: 0.5989; val_mae: 0.8283; [0.9210,1.9634,2.4010]\r\n",
      "Epoch 11/24\r\n",
      " - 22s - loss: 0.9717\r\n",
      "val_kappa: 0.5990; val_mae: 0.8351; [1.2021,1.6518,2.1383]\r\n",
      "Epoch 12/24\r\n",
      " - 21s - loss: 0.9496\r\n",
      "val_kappa: 0.6059; val_mae: 0.8234; [1.1327,1.7713,2.3654]\r\n",
      "Latest lr: 0.00050\r\n",
      "Reached 1 cycle\r\n",
      "Epoch 13/24\r\n",
      " - 21s - loss: 0.9381\r\n",
      "val_kappa: 0.6070; val_mae: 0.8108; [1.0308,1.7173,2.4439]\r\n",
      "Epoch 14/24\r\n",
      " - 21s - loss: 0.9464\r\n",
      "val_kappa: 0.6071; val_mae: 0.8183; [1.1705,1.5877,2.3754]\r\n",
      "Epoch 15/24\r\n",
      " - 21s - loss: 0.9465\r\n",
      "val_kappa: 0.5919; val_mae: 0.8250; [1.1217,1.7406,2.3429]\r\n",
      "Epoch 16/24\r\n",
      " - 21s - loss: 0.9468\r\n",
      "val_kappa: 0.5951; val_mae: 0.8469; [1.1783,1.8304,2.1794]\r\n",
      "Epoch 17/24\r\n",
      " - 21s - loss: 0.9751\r\n",
      "val_kappa: 0.6007; val_mae: 0.8234; [1.1957,1.4932,2.3017]\r\n",
      "Epoch 18/24\r\n",
      " - 21s - loss: 0.9853\r\n",
      "val_kappa: 0.6005; val_mae: 0.8033; [1.0466,1.8750,2.4608]\r\n",
      "Epoch 19/24\r\n",
      " - 21s - loss: 0.9720\r\n",
      "val_kappa: 0.6007; val_mae: 0.8228; [1.1216,1.4849,2.4136]\r\n",
      "Epoch 20/24\r\n",
      " - 22s - loss: 0.9581\r\n",
      "val_kappa: 0.5953; val_mae: 0.8197; [0.9957,1.8416,2.3775]\r\n",
      "Epoch 21/24\r\n",
      " - 21s - loss: 0.9431\r\n",
      "val_kappa: 0.5930; val_mae: 0.8164; [1.1054,1.7792,2.2475]\r\n",
      "Epoch 22/24\r\n",
      " - 21s - loss: 0.9287\r\n",
      "val_kappa: 0.6072; val_mae: 0.8017; [1.0156,1.8975,2.2989]\r\n",
      "Epoch 23/24\r\n",
      " - 21s - loss: 0.9135\r\n",
      "val_kappa: 0.5999; val_mae: 0.8091; [1.1215,1.7233,2.2853]\r\n",
      "Epoch 24/24\r\n",
      " - 21s - loss: 0.8911\r\n",
      "val_kappa: 0.6025; val_mae: 0.8110; [0.9103,1.7959,2.3634]\r\n",
      "Latest lr: 0.00050\r\n",
      "Reached 2 cycle\r\n",
      "2 0.606355101781592\r\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\r\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\r\n",
      "Epoch 1/24\r\n",
      " - 23s - loss: 1.8593\r\n",
      "val_kappa: 0.3807; val_mae: 1.1268; [1.0586,1.1683,2.5100]\r\n",
      "Epoch 2/24\r\n",
      " - 22s - loss: 1.2924\r\n",
      "val_kappa: 0.5540; val_mae: 1.0055; [1.2376,1.4370,1.6686]\r\n",
      "Epoch 3/24\r\n",
      " - 21s - loss: 1.2003\r\n",
      "val_kappa: 0.5908; val_mae: 0.9106; [1.3057,1.6179,1.8698]\r\n",
      "Epoch 4/24\r\n",
      " - 21s - loss: 1.1313\r\n",
      "val_kappa: 0.5830; val_mae: 0.8672; [1.0744,1.7077,2.0565]\r\n",
      "Epoch 5/24\r\n",
      " - 21s - loss: 1.1031\r\n",
      "val_kappa: 0.5941; val_mae: 0.8690; [1.1223,1.7844,2.0448]\r\n",
      "Epoch 6/24\r\n",
      " - 21s - loss: 1.0810\r\n",
      "val_kappa: 0.5930; val_mae: 0.8773; [1.0561,1.8235,2.0618]\r\n",
      "Epoch 7/24\r\n",
      " - 22s - loss: 1.0593\r\n",
      "val_kappa: 0.5989; val_mae: 0.8420; [0.7813,1.8734,2.2378]\r\n",
      "Epoch 8/24\r\n",
      " - 21s - loss: 1.0354\r\n",
      "val_kappa: 0.5978; val_mae: 0.8295; [1.0610,1.6889,2.1623]\r\n",
      "Epoch 9/24\r\n",
      " - 21s - loss: 1.0081\r\n",
      "val_kappa: 0.5946; val_mae: 0.8378; [1.0828,1.7688,2.2783]\r\n",
      "Epoch 10/24\r\n",
      " - 21s - loss: 0.9895\r\n",
      "val_kappa: 0.6044; val_mae: 0.8045; [1.0777,1.8674,2.3761]\r\n",
      "Epoch 11/24\r\n",
      " - 21s - loss: 0.9581\r\n",
      "val_kappa: 0.6035; val_mae: 0.8281; [1.0636,1.7702,2.2023]\r\n",
      "Epoch 12/24\r\n",
      " - 21s - loss: 0.9502\r\n",
      "val_kappa: 0.6089; val_mae: 0.8276; [1.0811,1.6938,2.1790]\r\n",
      "Latest lr: 0.00050\r\n",
      "Reached 1 cycle\r\n",
      "Epoch 13/24\r\n",
      " - 21s - loss: 0.9429\r\n",
      "val_kappa: 0.6041; val_mae: 0.8036; [1.0491,1.7984,2.2444]\r\n",
      "Epoch 14/24\r\n",
      " - 22s - loss: 0.9385\r\n",
      "val_kappa: 0.6135; val_mae: 0.8027; [1.1364,1.7068,2.2765]\r\n",
      "Epoch 15/24\r\n",
      " - 21s - loss: 0.9388\r\n",
      "val_kappa: 0.6085; val_mae: 0.8398; [1.1578,1.6461,2.1668]\r\n",
      "Epoch 16/24\r\n",
      " - 21s - loss: 0.9502\r\n",
      "val_kappa: 0.6067; val_mae: 0.8238; [1.0866,1.7120,2.3500]\r\n",
      "Epoch 17/24\r\n",
      " - 21s - loss: 0.9618\r\n",
      "val_kappa: 0.6096; val_mae: 0.8053; [0.8697,1.8012,2.3306]\r\n",
      "Epoch 18/24\r\n",
      " - 21s - loss: 0.9648\r\n",
      "val_kappa: 0.5987; val_mae: 0.8318; [1.2372,1.7948,2.3681]\r\n",
      "Epoch 19/24\r\n",
      " - 21s - loss: 0.9675\r\n",
      "val_kappa: 0.5999; val_mae: 0.8570; [1.2362,1.5178,2.2070]\r\n",
      "Epoch 20/24\r\n",
      " - 21s - loss: 0.9541\r\n",
      "val_kappa: 0.6034; val_mae: 0.8301; [1.0336,1.5856,2.3653]\r\n",
      "Epoch 21/24\r\n",
      " - 22s - loss: 0.9248\r\n",
      "val_kappa: 0.6106; val_mae: 0.8173; [0.9503,1.6255,2.1236]\r\n",
      "Epoch 22/24\r\n",
      " - 21s - loss: 0.9126\r\n",
      "val_kappa: 0.6007; val_mae: 0.8466; [1.0818,1.4717,2.2103]\r\n",
      "Epoch 23/24\r\n",
      " - 21s - loss: 0.9005\r\n",
      "val_kappa: 0.6076; val_mae: 0.8160; [1.1075,1.7109,2.3454]\r\n",
      "Epoch 24/24\r\n",
      " - 21s - loss: 0.8845\r\n",
      "val_kappa: 0.6070; val_mae: 0.8083; [1.0334,1.6427,2.3907]\r\n",
      "Latest lr: 0.00050\r\n",
      "Reached 2 cycle\r\n",
      "3 0.6085783688792752\r\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\r\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\r\n",
      "Epoch 1/24\r\n",
      " - 24s - loss: 1.8656\r\n",
      "val_kappa: 0.4677; val_mae: 1.0472; [1.2825,1.5046,1.7353]\r\n",
      "Epoch 2/24\r\n",
      " - 21s - loss: 1.2914\r\n",
      "val_kappa: 0.4895; val_mae: 1.0294; [1.0150,1.3325,2.5005]\r\n",
      "Epoch 3/24\r\n",
      " - 21s - loss: 1.1852\r\n",
      "val_kappa: 0.6017; val_mae: 0.8452; [1.1530,1.7728,2.2221]\r\n",
      "Epoch 4/24\r\n",
      " - 21s - loss: 1.1190\r\n",
      "val_kappa: 0.6111; val_mae: 0.8266; [1.0656,1.8203,2.2720]\r\n",
      "Epoch 5/24\r\n",
      " - 22s - loss: 1.0854\r\n",
      "val_kappa: 0.6184; val_mae: 0.8657; [1.1096,1.8996,2.2027]\r\n",
      "Epoch 6/24\r\n",
      " - 21s - loss: 1.0757\r\n",
      "val_kappa: 0.6054; val_mae: 0.8594; [1.0849,1.6418,2.2901]\r\n",
      "Epoch 7/24\r\n",
      " - 21s - loss: 1.0707\r\n",
      "val_kappa: 0.5989; val_mae: 0.8684; [1.1467,1.7613,2.1952]\r\n",
      "Epoch 8/24\r\n",
      " - 22s - loss: 1.0294\r\n",
      "val_kappa: 0.6140; val_mae: 0.8615; [0.8204,1.6318,2.1950]\r\n",
      "Epoch 9/24\r\n",
      " - 21s - loss: 1.0074\r\n",
      "val_kappa: 0.6183; val_mae: 0.8364; [0.9855,1.7843,2.3320]\r\n",
      "Epoch 10/24\r\n",
      " - 21s - loss: 0.9791\r\n",
      "val_kappa: 0.6179; val_mae: 0.8495; [1.0744,1.8002,2.2996]\r\n",
      "Epoch 11/24\r\n",
      " - 21s - loss: 0.9611\r\n",
      "val_kappa: 0.6154; val_mae: 0.8425; [1.0706,1.6905,2.3584]\r\n",
      "Epoch 12/24\r\n",
      " - 21s - loss: 0.9434\r\n",
      "val_kappa: 0.6214; val_mae: 0.8375; [1.1215,1.6668,2.2340]\r\n",
      "Latest lr: 0.00050\r\n",
      "Reached 1 cycle\r\n",
      "Epoch 13/24\r\n",
      " - 22s - loss: 0.9409\r\n",
      "val_kappa: 0.6185; val_mae: 0.8377; [1.1806,1.5599,2.3072]\r\n",
      "Epoch 14/24\r\n",
      " - 21s - loss: 0.9387\r\n",
      "val_kappa: 0.6177; val_mae: 0.8466; [1.1250,1.6984,2.2751]\r\n",
      "Epoch 15/24\r\n",
      " - 21s - loss: 0.9419\r\n",
      "val_kappa: 0.6153; val_mae: 0.8445; [1.1233,1.7153,2.3551]\r\n",
      "Epoch 16/24\r\n",
      " - 21s - loss: 0.9490\r\n",
      "val_kappa: 0.6207; val_mae: 0.8246; [1.1131,1.7834,2.2190]\r\n",
      "Epoch 17/24\r\n",
      " - 22s - loss: 0.9608\r\n",
      "val_kappa: 0.6146; val_mae: 0.8625; [1.1715,1.8278,2.3349]\r\n",
      "Epoch 18/24\r\n",
      " - 21s - loss: 0.9673\r\n",
      "val_kappa: 0.6090; val_mae: 0.8382; [1.0247,1.5815,2.3846]\r\n",
      "Epoch 19/24\r\n",
      " - 21s - loss: 0.9672\r\n",
      "val_kappa: 0.6096; val_mae: 0.8228; [1.1614,1.7295,2.2057]\r\n",
      "Epoch 20/24\r\n",
      " - 21s - loss: 0.9551\r\n",
      "val_kappa: 0.5980; val_mae: 0.8204; [1.1704,1.8197,2.1606]\r\n",
      "Epoch 21/24\r\n",
      " - 21s - loss: 0.9352\r\n",
      "val_kappa: 0.6123; val_mae: 0.8268; [1.1298,1.7749,2.2493]\r\n",
      "Epoch 22/24\r\n",
      " - 21s - loss: 0.9064\r\n",
      "val_kappa: 0.6154; val_mae: 0.8373; [1.1922,1.5651,2.2447]\r\n",
      "Epoch 23/24\r\n",
      " - 21s - loss: 0.9022\r\n",
      "val_kappa: 0.6193; val_mae: 0.8095; [1.0756,1.6838,2.3971]\r\n",
      "Epoch 24/24\r\n",
      " - 21s - loss: 0.8843\r\n",
      "val_kappa: 0.6222; val_mae: 0.8183; [1.1345,1.6750,2.4527]\r\n",
      "Latest lr: 0.00050\r\n",
      "Reached 2 cycle\r\n",
      "4 0.622345413858539\r\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\r\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\r\n",
      "Epoch 1/24\r\n",
      " - 23s - loss: 1.9749\r\n",
      "val_kappa: 0.4936; val_mae: 1.0195; [1.3236,1.4453,1.7860]\r\n",
      "Epoch 2/24\r\n",
      " - 22s - loss: 1.3041\r\n",
      "val_kappa: 0.6065; val_mae: 0.8131; [1.3966,1.9375,2.3161]\r\n",
      "Epoch 3/24\r\n",
      " - 21s - loss: 1.1766\r\n",
      "val_kappa: 0.6075; val_mae: 0.8182; [1.0112,1.9493,2.1270]\r\n",
      "Epoch 4/24\r\n",
      " - 22s - loss: 1.1179\r\n",
      "val_kappa: 0.6157; val_mae: 0.8424; [1.1528,1.7982,2.1231]\r\n",
      "Epoch 5/24\r\n",
      " - 21s - loss: 1.0986\r\n",
      "val_kappa: 0.5998; val_mae: 0.8819; [1.0917,1.7748,2.0984]\r\n",
      "Epoch 6/24\r\n",
      " - 21s - loss: 1.0694\r\n",
      "val_kappa: 0.6058; val_mae: 0.7981; [1.1331,1.7661,2.3652]\r\n",
      "Epoch 7/24\r\n",
      " - 22s - loss: 1.0680\r\n",
      "val_kappa: 0.5983; val_mae: 0.8404; [1.1747,1.1323,2.2283]\r\n",
      "Epoch 8/24\r\n",
      " - 21s - loss: 1.0360\r\n",
      "val_kappa: 0.6076; val_mae: 0.8157; [1.0876,1.6387,2.2166]\r\n",
      "Epoch 9/24\r\n",
      " - 21s - loss: 1.0097\r\n",
      "val_kappa: 0.6235; val_mae: 0.8166; [1.1604,1.6432,2.2826]\r\n",
      "Epoch 10/24\r\n",
      " - 21s - loss: 0.9848\r\n",
      "val_kappa: 0.6263; val_mae: 0.8045; [1.1357,1.7034,2.3231]\r\n",
      "Epoch 11/24\r\n",
      " - 22s - loss: 0.9723\r\n",
      "val_kappa: 0.6307; val_mae: 0.7724; [1.0881,1.7160,2.4064]\r\n",
      "Epoch 12/24\r\n",
      " - 22s - loss: 0.9543\r\n",
      "val_kappa: 0.6294; val_mae: 0.7848; [1.0811,1.5482,2.3931]\r\n",
      "Latest lr: 0.00050\r\n",
      "Reached 1 cycle\r\n",
      "Epoch 13/24\r\n",
      " - 21s - loss: 0.9539\r\n",
      "val_kappa: 0.6238; val_mae: 0.8144; [1.0980,1.7504,2.2558]\r\n",
      "Epoch 14/24\r\n",
      " - 21s - loss: 0.9548\r\n",
      "val_kappa: 0.6236; val_mae: 0.8008; [1.1112,1.6252,2.2975]\r\n",
      "Epoch 15/24\r\n",
      " - 21s - loss: 0.9472\r\n",
      "val_kappa: 0.6199; val_mae: 0.7964; [1.1141,1.7583,2.2626]\r\n",
      "Epoch 16/24\r\n",
      " - 22s - loss: 0.9522\r\n",
      "val_kappa: 0.6250; val_mae: 0.7799; [1.0967,1.7942,2.2325]\r\n",
      "Epoch 17/24\r\n",
      " - 21s - loss: 0.9729\r\n",
      "val_kappa: 0.6142; val_mae: 0.8121; [1.0931,1.8623,2.3201]\r\n",
      "Epoch 18/24\r\n",
      " - 21s - loss: 0.9792\r\n",
      "val_kappa: 0.6109; val_mae: 0.8351; [1.1390,1.6330,2.3026]\r\n",
      "Epoch 19/24\r\n",
      " - 21s - loss: 0.9865\r\n",
      "val_kappa: 0.6229; val_mae: 0.8312; [0.8280,1.4597,2.2362]\r\n",
      "Epoch 20/24\r\n",
      " - 21s - loss: 0.9683\r\n",
      "val_kappa: 0.6235; val_mae: 0.8051; [1.0895,1.6545,2.3676]\r\n",
      "Epoch 21/24\r\n",
      " - 21s - loss: 0.9455\r\n",
      "val_kappa: 0.6249; val_mae: 0.7751; [1.0648,1.5807,2.4744]\r\n",
      "Epoch 22/24\r\n",
      " - 21s - loss: 0.9386\r\n",
      "val_kappa: 0.6212; val_mae: 0.7662; [1.0967,1.7567,2.3185]\r\n",
      "Epoch 23/24\r\n",
      " - 21s - loss: 0.9253\r\n",
      "val_kappa: 0.6282; val_mae: 0.7995; [1.1004,1.6182,2.1650]\r\n",
      "Epoch 24/24\r\n",
      " - 22s - loss: 0.8998\r\n",
      "val_kappa: 0.6319; val_mae: 0.7794; [1.1481,1.7114,2.2582]\r\n",
      "Latest lr: 0.00050\r\n",
      "Reached 2 cycle\r\n",
      "5 0.6303790079377865\r\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\r\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\r\n",
      "Epoch 1/24\r\n",
      " - 23s - loss: 1.9192\r\n",
      "val_kappa: 0.4003; val_mae: 1.0850; [1.1207,1.3036,2.3969]\r\n",
      "Epoch 2/24\r\n",
      " - 21s - loss: 1.3063\r\n",
      "val_kappa: 0.5754; val_mae: 0.9202; [1.2639,1.5583,1.9057]\r\n",
      "Epoch 3/24\r\n",
      " - 22s - loss: 1.1802\r\n",
      "val_kappa: 0.5932; val_mae: 0.9488; [1.3038,1.6172,1.8805]\r\n",
      "Epoch 4/24\r\n",
      " - 21s - loss: 1.1270\r\n",
      "val_kappa: 0.6026; val_mae: 0.8603; [0.8311,1.5384,2.0805]\r\n",
      "Epoch 5/24\r\n",
      " - 21s - loss: 1.0920\r\n",
      "val_kappa: 0.5872; val_mae: 0.9023; [1.1619,1.8023,2.0914]\r\n",
      "Epoch 6/24\r\n",
      " - 21s - loss: 1.0711\r\n",
      "val_kappa: 0.6043; val_mae: 0.8940; [1.0952,1.7265,2.0372]\r\n",
      "Epoch 7/24\r\n",
      " - 21s - loss: 1.0516\r\n",
      "val_kappa: 0.6067; val_mae: 0.8456; [1.1959,1.5819,2.2954]\r\n",
      "Epoch 8/24\r\n",
      " - 21s - loss: 1.0235\r\n",
      "val_kappa: 0.6045; val_mae: 0.8460; [1.1172,1.5923,2.2691]\r\n",
      "Epoch 9/24\r\n",
      " - 21s - loss: 0.9956\r\n",
      "val_kappa: 0.6041; val_mae: 0.8451; [1.1521,1.7627,2.1885]\r\n",
      "Epoch 10/24\r\n",
      " - 21s - loss: 0.9765\r\n",
      "val_kappa: 0.6112; val_mae: 0.8395; [1.1250,1.6731,2.3311]\r\n",
      "Epoch 11/24\r\n",
      " - 21s - loss: 0.9657\r\n",
      "val_kappa: 0.6093; val_mae: 0.8393; [1.0570,1.7946,2.2869]\r\n",
      "Epoch 12/24\r\n",
      " - 21s - loss: 0.9395\r\n",
      "val_kappa: 0.6066; val_mae: 0.8307; [1.2001,1.7066,2.2778]\r\n",
      "Latest lr: 0.00050\r\n",
      "Reached 1 cycle\r\n",
      "Epoch 13/24\r\n",
      " - 21s - loss: 0.9333\r\n",
      "val_kappa: 0.6091; val_mae: 0.8350; [1.1464,1.5675,2.3025]\r\n",
      "Epoch 14/24\r\n",
      " - 23s - loss: 0.9311\r\n",
      "val_kappa: 0.6143; val_mae: 0.8365; [0.9894,1.7620,2.2782]\r\n",
      "Epoch 15/24\r\n",
      " - 21s - loss: 0.9323\r\n",
      "val_kappa: 0.6125; val_mae: 0.8444; [1.1331,1.6856,2.3145]\r\n",
      "Epoch 16/24\r\n",
      " - 21s - loss: 0.9470\r\n",
      "val_kappa: 0.6066; val_mae: 0.8593; [1.1975,1.6616,2.2541]\r\n",
      "Epoch 17/24\r\n",
      " - 22s - loss: 0.9496\r\n",
      "val_kappa: 0.6075; val_mae: 0.8046; [0.9852,1.6972,2.4871]\r\n",
      "Epoch 18/24\r\n",
      " - 21s - loss: 0.9790\r\n",
      "val_kappa: 0.6032; val_mae: 0.8457; [1.1420,1.7660,2.3078]\r\n",
      "Epoch 19/24\r\n",
      " - 21s - loss: 0.9718\r\n",
      "val_kappa: 0.6250; val_mae: 0.7933; [0.9906,1.4759,2.4959]\r\n",
      "Epoch 20/24\r\n",
      " - 21s - loss: 0.9502\r\n",
      "val_kappa: 0.6064; val_mae: 0.8042; [1.0910,1.8046,2.5101]\r\n",
      "Epoch 21/24\r\n",
      " - 21s - loss: 0.9385\r\n",
      "val_kappa: 0.6042; val_mae: 0.8305; [1.1404,1.7838,2.2253]\r\n",
      "Epoch 22/24\r\n",
      " - 21s - loss: 0.9191\r\n",
      "val_kappa: 0.6105; val_mae: 0.8327; [1.1188,1.7310,2.2212]\r\n",
      "Epoch 23/24\r\n",
      " - 21s - loss: 0.9000\r\n",
      "val_kappa: 0.6136; val_mae: 0.8019; [0.9436,1.8632,2.4509]\r\n",
      "Epoch 24/24\r\n",
      " - 21s - loss: 0.8828\r\n",
      "val_kappa: 0.6084; val_mae: 0.8201; [1.0548,1.7475,2.3496]\r\n",
      "Latest lr: 0.00050\r\n",
      "Reached 2 cycle\r\n",
      "6 0.6105921222971056\r\n",
      "0.6129703371172324\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python train_model.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
